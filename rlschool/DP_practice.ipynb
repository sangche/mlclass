{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangche/mlclass/blob/master/rlschool/DP_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr2tPjlrf-rP"
      },
      "source": [
        "# Practice Session 1: Policy and Value Iteration\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rlsummerschool/practical-sessions/blob/master/notebooks/DP_practice.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "mccFWYAoBkX5",
        "outputId": "d5e1df66-898e-4f78-e2fe-22f0f00d7470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/etc/alternatives/python3 --version"
      ],
      "metadata": {
        "id": "DZTKLEEKCPsC",
        "outputId": "1660baef-f79a-41e1-91c7-94a6245a076f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python3.10"
      ],
      "metadata": {
        "id": "NIi9y4fPDETF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!which python3.10"
      ],
      "metadata": {
        "id": "XW-_aXivB2YZ",
        "outputId": "59eb9a62-1936-4d34-e168-00cb84cc897c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -sf /usr/bin/python3.10 /etc/alternatives/python3"
      ],
      "metadata": {
        "id": "XDuNfET9Ql_y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /etc/alternatives/python3"
      ],
      "metadata": {
        "id": "KfXQdUP-R6vG",
        "outputId": "25af7f0e-f557-4da1-adfa-68475411c350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root 19 Feb 25 04:03 /etc/alternatives/python3 -> /usr/bin/python3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "RdNs4KDARwio",
        "outputId": "9b8bbe6c-7f15-466c-bb90-b7011b809930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "VYZ_hFDkXuY0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install python3-pip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pWmEYkCBVe8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --version"
      ],
      "metadata": {
        "id": "5n3xfnlSSaxL",
        "outputId": "9514cf42-214d-41ff-fa27-21369c505755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFtCdzEI9gHZ"
      },
      "source": [
        "## RL development in Python\n",
        "\n",
        "Large RL projects are usually developed locally, as any other Python package, in OOP, version controlled by Git (see [example](https://github.com/rlsummerschool/practical-sessions/blob/master/rlss_practice/environments.py)).\n",
        "\n",
        "[Jupyter](https://jupyter.org/) notebooks, like this one, are collections of text cells and code cells, that can be executed with an interactive interpreter.\n",
        "Colab notebooks are Jupyter notebooks whose interpreter runs in a runtime that is hosted by Google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqPq-DrFG3se"
      },
      "source": [
        "### Environments and tasks\n",
        "\n",
        "An MDP is a model of Reinforcement Learning tasks. It comprises of:\n",
        "* a **state space** $\\mathcal{S}$\n",
        "* a **action space** $\\mathcal{A}$\n",
        "* a **starting-state distribution** $\\nu_0(s)$\n",
        "* a **reward function** $r: \\mathcal{S}\\times\\mathcal{A}\\rightarrow [0,1]$ (or vector $r\\in[0,1]^{|\\mathcal{S}|\\cdot|\\mathcal{A}|}$)\n",
        "* a **transition function** $p: \\mathcal{S}\\times\\mathcal{A}\\rightarrow \\Delta_{\\mathcal{S}}$ (or matrix $p \\in\\mathbb{R}^{|\\mathcal{S}|\\cdot|\\mathcal{A}|\\times|\\mathcal{S}|}$)\n",
        "\n",
        "where $\\Delta_{\\mathcal{S}} = \\{q\\in\\mathbb{R}^{|\\mathcal{S}|} \\,\\,|\\,\\, \\sum_{s\\in\\mathcal{S}}q(s) = 1,\\, q(s)\\geq 0 \\text{ for } s\\in\\mathcal{S}\\}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsQticuSJmdO"
      },
      "source": [
        "Interaction between an agent (or decision maker) and some environment (or task) in some round $t$:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11KyhHxuaileBEJ9fFZmIS5-GzncQgyqt\" alt=\"agent-environment interaction\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "The agent is equipped with a policy $\\pi$ mapping states to actions (or a distribution over actions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob8WriuZLh-g"
      },
      "source": [
        "### The Gym(nasium) interface\n",
        "\n",
        "[Gymnasium](https://gymnasium.farama.org/) is a standard API for Decision Processes, based on OpenAI's Gym library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JgaqoJNTOi6o"
      },
      "outputs": [],
      "source": [
        "# Standard import\n",
        "import gymnasium as gym\n",
        "from gymnasium import Env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDwwk5BL1eRQ"
      },
      "source": [
        "The library also defines a number of classic benchmarks [Atari](https://gymnasium.farama.org/environments/atari/) games, [MuJoCo](https://gymnasium.farama.org/environments/mujoco/) simulations, and [ToyText](https://gymnasium.farama.org/environments/toy_text/), minimal environments for debugging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DlzPIsH23hpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775798d9-e761-470b-b7e1-2d0d413d7846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n",
            "Discrete(48)\n"
          ]
        }
      ],
      "source": [
        "# A registered environment (for example Cliff Walking) can be instantiated with\n",
        "env = gym.make(\"CliffWalking-v0\")\n",
        "\n",
        "# Important properties:\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXhGGkAI7XEh"
      },
      "source": [
        "We only consider discrete state and action spaces. See the possible alternatives: `dir(gym.spaces)`\n",
        "\n",
        "Environments can be fully observable, partially observable or non-stationary. All these variants fit the same environment interface.\n",
        "We should know which class our environment belongs to.\n",
        "Today, we only consider stationary and fully observable MDPs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNcySyOGACgZ"
      },
      "source": [
        "The environment interface allows  to sample initial states and transition with `env.reset()` and `env.step(action)`. Every method is well documented (see `help(gym.Env.step)`).\n",
        "\n",
        "Try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Wnymj388egTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1cd352-8ec9-4e6c-bc03-dbf1248d7262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: 36\n",
            "Observation: 24, reward -1\n",
            "Observation: 12, reward -1\n",
            "Observation: 0, reward -1\n",
            "Observation: 1, reward -1\n",
            "Observation: 13, reward -1\n"
          ]
        }
      ],
      "source": [
        "observation, info = env.reset()\n",
        "print(f\"Initial observation: {observation}\")\n",
        "\n",
        "action = 0  # any action (Up)\n",
        "\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Observation: {observation}, reward {reward}\")\n",
        "\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Observation: {observation}, reward {reward}\")\n",
        "\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Observation: {observation}, reward {reward}\")\n",
        "\n",
        "observation, reward, terminated, truncated, info = env.step(1) # Right\n",
        "print(f\"Observation: {observation}, reward {reward}\")\n",
        "\n",
        "observation, reward, terminated, truncated, info = env.step(2) # Down\n",
        "print(f\"Observation: {observation}, reward {reward}\")\n",
        "\n",
        "# see picture below link\n",
        "# https://www.gymlibrary.dev/environments/toy_text/cliff_walking/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xffKa7UfiOC"
      },
      "source": [
        "Custom environments can be created by subclassing the `Env` class.\n",
        "\n",
        "Environments can be also modified with wrappers. See the predefined Gymnasium [Wrappers](https://gymnasium.farama.org/api/wrappers/#gymnasium-wrappers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHgub_3DH-9V"
      },
      "source": [
        "## Setting up our RL task\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QotsgLalKv4"
      },
      "source": [
        "We experiment with a simple grid-world environment, based on the implementation in [minigrid](https://minigrid.farama.org/environments/minigrid/). The environment is configured and observations are transformed appropriately (you already know how)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/rlsummerschool/practical-sessions"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DSY52Vt6nevD",
        "outputId": "ffb80648-b606-473e-e05c-ee9456e4a6e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/rlsummerschool/practical-sessions\n",
            "  Cloning https://github.com/rlsummerschool/practical-sessions to /tmp/pip-req-build-1jfmsyr3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/rlsummerschool/practical-sessions /tmp/pip-req-build-1jfmsyr3\n",
            "  Resolved https://github.com/rlsummerschool/practical-sessions to commit 4f1ee290aa22d1c170c88fe234e5f448cf59a8db\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from rlss_practice==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: minigrid<3.0.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from rlss_practice==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: moviepy<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from rlss_practice==0.1.0) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (4.56.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (11.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (1.4.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (2.4.7)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from minigrid<3.0.0,>=2.2.1->rlss_practice==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from minigrid<3.0.0,>=2.2.1->rlss_practice==0.1.0) (2.6.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (2.37.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid<3.0.0,>=2.2.1->rlss_practice==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid<3.0.0,>=2.2.1->rlss_practice==0.1.0) (0.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid<3.0.0,>=2.2.1->rlss_practice==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.1->rlss_practice==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy<2.0.0,>=1.0.3->rlss_practice==0.1.0) (3.4.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sZ94hdAiozEj",
        "outputId": "e0dacffe-eb0d-4303-b95a-a024c938aad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package              Version\n",
            "-------------------- -------------\n",
            "blinker              1.4\n",
            "certifi              2025.1.31\n",
            "charset-normalizer   3.4.1\n",
            "cloudpickle          3.1.1\n",
            "contourpy            1.3.1\n",
            "cryptography         3.4.8\n",
            "cycler               0.12.1\n",
            "dbus-python          1.2.18\n",
            "decorator            4.4.2\n",
            "distro               1.7.0\n",
            "Farama-Notifications 0.0.4\n",
            "fonttools            4.56.0\n",
            "gymnasium            1.0.0\n",
            "httplib2             0.20.2\n",
            "idna                 3.10\n",
            "imageio              2.37.0\n",
            "imageio-ffmpeg       0.6.0\n",
            "importlib-metadata   4.6.4\n",
            "jeepney              0.7.1\n",
            "keyring              23.5.0\n",
            "kiwisolver           1.4.8\n",
            "launchpadlib         1.10.16\n",
            "lazr.restfulclient   0.14.4\n",
            "lazr.uri             1.0.6\n",
            "matplotlib           3.10.0\n",
            "minigrid             2.5.0\n",
            "more-itertools       8.10.0\n",
            "moviepy              1.0.3\n",
            "numpy                2.2.3\n",
            "oauthlib             3.2.0\n",
            "packaging            24.2\n",
            "pillow               11.1.0\n",
            "pip                  22.0.2\n",
            "proglog              0.1.10\n",
            "pygame               2.6.1\n",
            "PyGObject            3.42.1\n",
            "PyJWT                2.3.0\n",
            "pyparsing            2.4.7\n",
            "python-apt           2.4.0+ubuntu4\n",
            "python-dateutil      2.9.0.post0\n",
            "requests             2.32.3\n",
            "rlss_practice        0.1.0\n",
            "SecretStorage        3.3.1\n",
            "setuptools           59.6.0\n",
            "six                  1.16.0\n",
            "tqdm                 4.67.1\n",
            "typing_extensions    4.12.2\n",
            "urllib3              2.3.0\n",
            "wadllib              1.3.6\n",
            "wheel                0.37.1\n",
            "zipp                 1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Environment configurations, maps and transition functions\"\"\"\n",
        "\n",
        "import itertools\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from typing import SupportsFloat\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium.wrappers.time_limit import TimeLimit\n",
        "from minigrid import envs\n",
        "from minigrid.core.constants import DIR_TO_VEC\n",
        "from minigrid.minigrid_env import MiniGridEnv\n",
        "\n",
        "from rlss_practice.wrappers import GoalMDP, DecodeObservation, FailProbability\n",
        "\n",
        "\n",
        "class MinigridBase(gym.Wrapper):\n",
        "    \"\"\"Base class for minigrid environments with explicit transition and reward functions.\n",
        "\n",
        "    The agent is rewarded with a 1 for every action executed at the goal state,\n",
        "    after which a sink failure state is reached.\n",
        "\n",
        "    Action space:\n",
        "\n",
        "    | Num | Name         | Action       |\n",
        "    |-----|--------------|--------------|\n",
        "    | 0   | left         | Turn left    |\n",
        "    | 1   | right        | Turn right   |\n",
        "    | 2   | forward      | Move forward |\n",
        "\n",
        "    Observation space:\n",
        "\n",
        "    | Name | Description             |\n",
        "    |------|-------------------------|\n",
        "    | x    | x coordinate            |\n",
        "    | y    | y coordinate (downward) |\n",
        "    | dir  | cardinal direction      |\n",
        "\n",
        "    The transition function is stored in `T`,\n",
        "    where `T[state][action][next_state]` is the transition probability.\n",
        "    The reward function is `R`. `R[state][action]` contains a reward.\n",
        "    \"\"\"\n",
        "\n",
        "    StateT = tuple[int, int, int]\n",
        "    ActionT = int\n",
        "\n",
        "    def __init__(self, minigrid: MiniGridEnv, seed: int, failure=0.0):\n",
        "        \"\"\"Initialize.\n",
        "\n",
        "        minigrid: an instantiated minigrid environment.\n",
        "        seed: random seed\n",
        "        failure: failure probability of the actions (another action is executed instead).\n",
        "        \"\"\"\n",
        "        # Store\n",
        "        self.minigrid = minigrid\n",
        "        self.minigrid.highlight = False\n",
        "        self.minigrid.action_space = gym.spaces.Discrete(3)\n",
        "        self.failure = failure\n",
        "        self.seed = seed\n",
        "\n",
        "        # Transform and store\n",
        "        env: gym.Env = FailProbability(self.minigrid, failure=failure, seed=seed)\n",
        "        env = DecodeObservation(env=env)\n",
        "        env = GoalMDP(env=env)\n",
        "        env = TimeLimit(env=env, max_episode_steps=50)\n",
        "        super().__init__(env=env)\n",
        "\n",
        "        # The grid must be generated once\n",
        "        env.reset(seed=self.seed)  # this creates the grid\n",
        "        self._initial_pos = minigrid.agent_pos\n",
        "        self._initial_dir = minigrid.agent_dir\n",
        "\n",
        "        # Do not generate it afterwards, only restore position\n",
        "        def _reset(self2: MiniGridEnv, *, seed=None, options=None):\n",
        "            self2.agent_pos = self._initial_pos\n",
        "            self2.agent_dir = self._initial_dir\n",
        "            self2.carrying = None\n",
        "            self2.step_count = 0\n",
        "            return minigrid.gen_obs(), {}\n",
        "\n",
        "        minigrid.reset = _reset.__get__(minigrid)\n",
        "        self._grid = (\n",
        "            self.minigrid.grid.encode()\n",
        "        )  # Just to check that the grid never changes\n",
        "\n",
        "        # States and actions\n",
        "        assert isinstance(env.observation_space, gym.spaces.MultiDiscrete)\n",
        "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "        obs_space = env.observation_space.nvec\n",
        "        self.actions = list(range(env.action_space.n))\n",
        "        self.states = list(\n",
        "            itertools.product(*(range(obs_space[i]) for i in range(len(obs_space))))\n",
        "        )\n",
        "        self.states = [\n",
        "            (x, y, o) for (x, y, o) in self.states if self._is_valid_position(x, y)\n",
        "        ]\n",
        "        self.sink_state = (0, 0, 0)\n",
        "        self.states.append(self.sink_state)\n",
        "\n",
        "        # Explicit transition and rewards functions\n",
        "        self._compute_model()\n",
        "\n",
        "    def step(self, action: int):\n",
        "        if action < 0 or action > 2:\n",
        "            raise RuntimeError(f\"Illegal action {action}. The action space is {self.action_space}\")\n",
        "\n",
        "        return super().step(action)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Simplified to string.\"\"\"\n",
        "        OBJECT_TO_STR = {\n",
        "            \"wall\": \"W\",\n",
        "            \"goal\": \"G\",\n",
        "        }\n",
        "        AGENT_DIR_TO_STR = {0: \">\", 1: \"V\", 2: \"<\", 3: \"^\"}\n",
        "\n",
        "        output = \"\"\n",
        "\n",
        "        for j in range(self.grid.height):\n",
        "            for i in range(self.grid.width):\n",
        "                if i == self.agent_pos[0] and j == self.agent_pos[1]:\n",
        "                    output += AGENT_DIR_TO_STR[self.agent_dir]\n",
        "                    continue\n",
        "\n",
        "                tile = self.grid.get(i, j)\n",
        "                if tile is None:\n",
        "                    output += \" \"\n",
        "                    continue\n",
        "                output += OBJECT_TO_STR[tile.type]\n",
        "\n",
        "            if j < self.grid.height - 1:\n",
        "                output += \"\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _is_valid_position(self, i: int, j: int) -> bool:\n",
        "        \"\"\"Testing whether a coordinate is a valid location.\"\"\"\n",
        "        if i < 0:\n",
        "            return False\n",
        "        if j < 0:\n",
        "            return False\n",
        "        if i >= self.minigrid.width:\n",
        "            return False\n",
        "        if j >= self.minigrid.height:\n",
        "            return False\n",
        "        cell = self.minigrid.grid.get(i, j)\n",
        "        if cell is not None and not cell.can_overlap():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _state_step(self, state: StateT, action: ActionT) -> StateT:\n",
        "        \"\"\"Utility to move states one step forward, no side effect.\"\"\"\n",
        "        x, y, direction = state\n",
        "\n",
        "        # Default transition to the sink failure state\n",
        "        assert self._is_valid_position(x, y)\n",
        "\n",
        "        # Transition left\n",
        "        if action == self.minigrid.actions.left:\n",
        "            direction -= 1\n",
        "            if direction < 0:\n",
        "                direction += 4\n",
        "            return x, y, direction\n",
        "\n",
        "        # Transition right\n",
        "        elif action == self.minigrid.actions.right:\n",
        "            direction = (direction + 1) % 4\n",
        "            return x, y, direction\n",
        "\n",
        "        # Transition forward\n",
        "        elif action == self.minigrid.actions.forward:\n",
        "            fwd_pos = np.array((x, y)) + DIR_TO_VEC[direction]\n",
        "            if self._is_valid_position(*fwd_pos):\n",
        "                return (fwd_pos[0], fwd_pos[1], direction)\n",
        "            else:\n",
        "                return state\n",
        "        # Error\n",
        "        else:\n",
        "            assert False, \"Invalid action\"\n",
        "\n",
        "    def _compute_model(self):\n",
        "        \"\"\"Compute explicit transition and reward functions for this environment.\"\"\"\n",
        "        # Compute matrices\n",
        "        T: dict = defaultdict(lambda: defaultdict())\n",
        "        R: dict = defaultdict(lambda: defaultdict())\n",
        "        for state in self.states:\n",
        "            for action in self.actions:\n",
        "                pos = self.minigrid.grid.get(state[0], state[1])\n",
        "                at_goal = pos is not None and pos.type == \"goal\"\n",
        "\n",
        "                # Reward\n",
        "                R[state][action] = 1.0 if at_goal else 0.0\n",
        "\n",
        "                # Transition\n",
        "                if at_goal or state == self.sink_state:\n",
        "                    T[state][action] = {s: 1.0 if s == self.sink_state else 0.0 for s in self.states}\n",
        "                else:\n",
        "                    success_state = self._state_step(state, action)\n",
        "                    failure_states = [\n",
        "                        self._state_step(state, a) for a in self.actions if a != action\n",
        "                    ]\n",
        "                    T[state][action] = {\n",
        "                        s: 1 - self.failure\n",
        "                        if s == success_state\n",
        "                        else self.failure / len(failure_states)\n",
        "                        if s in failure_states\n",
        "                        else 0.0\n",
        "                        for s in self.states\n",
        "                    }\n",
        "\n",
        "            T[state] = dict(T[state])\n",
        "            R[state] = dict(R[state])\n",
        "        self.T = dict(T)\n",
        "        self.R = dict(R)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Environment reset.\"\"\"\n",
        "        ret = super().reset(**kwargs)\n",
        "        assert (\n",
        "            self.minigrid.grid.encode() == self._grid\n",
        "        ).all(), \"The grid changed: this shouldn't happen\"\n",
        "        return ret\n",
        "\n",
        "    def pretty_print_T(self):\n",
        "        \"\"\"Prints the positive components of the transition function.\"\"\"\n",
        "        print(\"Transition function -- self.T\")\n",
        "        for state in self.states:\n",
        "            if self._is_valid_position(state[0], state[1]):\n",
        "                print(f\"State {state}\")\n",
        "                for action in self.actions:\n",
        "                    print(f\"  action {action}\")\n",
        "                    for state2 in self.states:\n",
        "                        if self.T[state][action][state2] > 0.0:\n",
        "                            print(\n",
        "                                f\"    next state {state2}: {self.T[state][action][state2]}\"\n",
        "                            )\n",
        "\n",
        "\n",
        "class Room(MinigridBase):\n",
        "    \"\"\"Single room environment with explicit model.\"\"\"\n",
        "\n",
        "    def __init__(self, failure=0.0, **kwargs):\n",
        "        \"\"\"Initialize.\n",
        "\n",
        "        failure: failure probability of the actions (another action is executed instead).\n",
        "        agent_start_pos: tuple with coordinates\n",
        "        agent_start_dir: north or..\n",
        "        size: room side length\n",
        "        \"\"\"\n",
        "        minigrid = envs.EmptyEnv(render_mode=\"rgb_array\", **kwargs)\n",
        "        super().__init__(minigrid=minigrid, seed=91273192, failure=failure)\n",
        "\n",
        "\n",
        "class Rooms(MinigridBase):\n",
        "    \"\"\"Grid-world with multple rooms and explicit model.\"\"\"\n",
        "\n",
        "    def __init__(self, rooms: int, size: int, failure=0.0, **kwargs):\n",
        "        \"\"\"Initialize.\n",
        "\n",
        "        rooms: how many rooms\n",
        "        size: maximum room size\n",
        "        failure: failure probability of the actions (another action is executed instead).\n",
        "        \"\"\"\n",
        "        # Initialize\n",
        "        minigrid = envs.MultiRoomEnv(\n",
        "            render_mode=\"rgb_array\", minNumRooms=rooms, maxNumRooms=rooms, maxRoomSize=size, screen_size=1500, **kwargs\n",
        "        )\n",
        "        super().__init__(minigrid=minigrid, seed=91273187, failure=failure)\n",
        "\n",
        "        # Remove doors and keys\n",
        "        minigrid.grid.grid = [\n",
        "            obj if obj is None or obj.type in (\"wall\", \"goal\") else None\n",
        "            for obj in minigrid.grid.grid\n",
        "        ]\n",
        "        self._grid = self.minigrid.grid.encode()\n",
        "\n",
        "\n",
        "def test(env: gym.Env, interactive: bool = False):\n",
        "    \"\"\"Environment rollouts with uniform policy for visualization.\n",
        "\n",
        "    env: gym environment to test\n",
        "    interactive: if True, the user selects the action\n",
        "    \"\"\"\n",
        "    print(f\"Action space: {env.action_space}\")\n",
        "    print(f\"Observation space: {env.observation_space}\")\n",
        "\n",
        "    def log():\n",
        "        env.render()\n",
        "        print(\"Env step\")\n",
        "        print(\"       Action:\", action)\n",
        "        print(\"  Observation:\", observation)\n",
        "        print(\"       Reward:\", reward)\n",
        "        print(\n",
        "            \"         Done:\",\n",
        "            \"terminated\" if terminated else \"truncated\" if truncated else \"False\",\n",
        "        )\n",
        "        print(\"         Info:\", info)\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    reward: SupportsFloat = 0.0\n",
        "    action = None\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    try:\n",
        "        observation, info = env.reset()\n",
        "        log()\n",
        "        while True:\n",
        "            # Action selection\n",
        "            action = env.action_space.sample()\n",
        "            if interactive:\n",
        "                a = input(f\"       Action (default {action}): \")\n",
        "                if a:\n",
        "                    action = int(a)\n",
        "                if action < 0:\n",
        "                    truncated = True\n",
        "\n",
        "            # Step\n",
        "            if action >= 0:\n",
        "                observation, reward, terminated, truncated, info = env.step(action)\n",
        "                log()\n",
        "\n",
        "            # Reset\n",
        "            if terminated or truncated:\n",
        "                observation, info = env.reset()\n",
        "                terminated = False\n",
        "                truncated = False\n",
        "                reward = 0.0\n",
        "                log()\n",
        "    finally:\n",
        "        env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = Room(\n",
        "        failure=0.0,\n",
        "        agent_start_pos=(1, 1),\n",
        "        agent_start_dir=0,\n",
        "        size=5,\n",
        "    )\n",
        "    test(env, interactive=True)"
      ],
      "metadata": {
        "id": "Q74-05lUtYPl",
        "outputId": "4cc9b8a8-39d9-4044-8883-18524092b174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gymnasium.wrappers.time_limit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-f55998be0946>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_limit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeLimit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mminigrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mminigrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDIR_TO_VEC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium.wrappers.time_limit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dzPxVxymjlZr",
        "outputId": "71e6056d-8c24-4d1d-9ca3-ab9c0c7f4391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rlss_practice'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-bbfe77ae8fef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrlss_practice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinigridBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initializing the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m env = Room(\n\u001b[1;32m      5\u001b[0m     \u001b[0mfailure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rlss_practice'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from rlss_practice.environments import Room, MinigridBase\n",
        "\n",
        "# Initializing the environment\n",
        "env = Room(\n",
        "    failure=0.0,\n",
        "    agent_start_pos=(1, 1),\n",
        "    agent_start_dir=0,\n",
        "    size=5,\n",
        ")\n",
        "print(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA74pMHsl_be"
      },
      "source": [
        "In this ASCII representation, > is the agent, facing right, G is the goal, and W are walls.\n",
        "\n",
        "Understand you own environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pY4EV-n0mH-d",
        "outputId": "03fb383a-8d3d-4508-de41-de525297189e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space: Discrete(4)\n",
            "Observation space: Discrete(48)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Action space:\", env.action_space)\n",
        "print(\"Observation space:\", env.observation_space, end=\"\\n\\n\")\n",
        "#print(help(MinigridBase))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk3VBz-ZmsvM"
      },
      "source": [
        "When you checkout `help(env)`:\n",
        "\n",
        "    class MinigridBase\n",
        "       MinigridBase(minigrid: minigrid.minigrid_env.MiniGridEnv, seed: int, failure=0.0)\n",
        "\n",
        "       Base class for minigrid environments with explicit transition and reward functions.\n",
        "\n",
        "       The agent is rewarded upon reaching the goal location.\n",
        "\n",
        "       Action space:\n",
        "\n",
        "       | Num | Name         | Action       |\n",
        "       |-----|--------------|--------------|\n",
        "       | 0   | left         | Turn left    |\n",
        "       | 1   | right        | Turn right   |\n",
        "       | 2   | forward      | Move forward |\n",
        "\n",
        "       Observation space:\n",
        "\n",
        "       | Name | Description             |\n",
        "       |------|-------------------------|\n",
        "       | x    | x coordinate            |\n",
        "       | y    | y coordinate (downward) |\n",
        "       | dir  | cardinal direction      |\n",
        "\n",
        "       The transition function is stored in `T`,\n",
        "       where `T[state][action][next_state]` is the transition probability.\n",
        "       The reward function is `R`. `R[state][action]` contains a reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOD43HaO2M6X"
      },
      "source": [
        "### Demo\n",
        "\n",
        "Here's a rollout loop, for a single trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ugRanvqJo4RW",
        "outputId": "2015f270-d8c0-45fe-901f-d99379bef04e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial observation: 36\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 0\n",
            "Observation: 24, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 0\n",
            "Observation: 12, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 0\n",
            "Observation: 0, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 1, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 2, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 2\n",
            "Observation: 14, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 2\n",
            "Observation: 26, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 27, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 28, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 2\n",
            "Observation: 36, reward -100, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 2\n",
            "Observation: 36, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 36, reward -100, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 36, reward -100, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 0\n",
            "Observation: 24, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 25, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 26, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 27, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 28, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 29, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 30, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 31, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 32, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 33, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 34, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 35, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 1\n",
            "Observation: 35, reward -1, terminated False, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n",
            "Action: 2\n",
            "Observation: 47, reward -1, terminated True, truncated False\n",
            "<OrderEnforcing<PassiveEnvChecker<CliffWalkingEnv<CliffWalking-v0>>>>\n"
          ]
        }
      ],
      "source": [
        "# Test it\n",
        "done = False\n",
        "observation, info = env.reset()\n",
        "print(\"Initial observation:\", observation)\n",
        "print(env)\n",
        "\n",
        "# Steps\n",
        "while not done:\n",
        "\n",
        "    try:\n",
        "      # Action selection\n",
        "\n",
        "      action = int(input(\"Action: \"))\n",
        "\n",
        "      # Transition\n",
        "      observation, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "\n",
        "      print(f\"Observation: {observation}, reward {reward}, terminated {terminated}, truncated {truncated}\")\n",
        "      print(env)\n",
        "\n",
        "    except:\n",
        "      done = True\n",
        "      print(\"Invalid action or KeyboardInterrupt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X910eylycXEf"
      },
      "source": [
        "We can define a function that performs `n_trajectories` rollouts on the environment with a given policy. We can also compute arbitrary statistics in the meanwhile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "gU1XQ-ObDqCn"
      },
      "outputs": [],
      "source": [
        "def rollouts(env, policy, n_trajectories, gamma):\n",
        "    \"\"\"Execute policy over env for n_trajectories and compute discounted return.\"\"\"\n",
        "    total_return = 0.0\n",
        "\n",
        "    # Trajectores\n",
        "    for _ in range(n_trajectories):\n",
        "\n",
        "        # Init\n",
        "        discount = 1.0\n",
        "        ret = 0.0\n",
        "        observation, info = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # Steps\n",
        "        while not done:\n",
        "\n",
        "            # Action selection\n",
        "            action = policy(observation)\n",
        "\n",
        "            # Transition\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            ret += reward * discount\n",
        "            discount *= gamma\n",
        "\n",
        "            if done:\n",
        "                total_return += ret\n",
        "\n",
        "    env.close()\n",
        "    return total_return / n_trajectories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmYQPLghM8cy"
      },
      "source": [
        "Since we don't have a policy yet, let's define the uniform one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Itk9LgVqL-DD"
      },
      "outputs": [],
      "source": [
        "class UniformPolicy():\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def __call__(self, observation):\n",
        "        return random.randint(0, self.n_actions-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQmcdqKt31XT"
      },
      "source": [
        "Let's try:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "qXnOiHr234mn",
        "outputId": "e524858c-2b5a-4307-d031-969f48f3b500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-152.48070350686254\n"
          ]
        }
      ],
      "source": [
        "avg_return = rollouts(env=env,\n",
        "                      policy=UniformPolicy(env.action_space.n),\n",
        "                      n_trajectories=20,\n",
        "                      gamma=0.9)\n",
        "\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpv9bGrwdtFd"
      },
      "source": [
        "We can also visualize the execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "k7_UY2RPOb7v",
        "outputId": "3031eca7-56bb-4066-af69-228dce9017b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rlss_practice'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-1973f25657c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrlss_practice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvisible_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRenderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rlss_practice'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from rlss_practice.wrappers import Renderer\n",
        "visible_env = Renderer(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2CieU7A_40i"
      },
      "outputs": [],
      "source": [
        "avg_return = rollouts(env=visible_env,\n",
        "                      policy=UniformPolicy(env.action_space.n),\n",
        "                      n_trajectories=1,\n",
        "                      gamma=0.9)\n",
        "\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbdR7lQQXhXu"
      },
      "outputs": [],
      "source": [
        "visible_env.play()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16XxGoHK-9zb"
      },
      "source": [
        "Finally, a small utility for creating callable objects from state-action mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4KDwEt06qnHI"
      },
      "outputs": [],
      "source": [
        "def make_policy(policy_dict):\n",
        "  \"\"\"Return the policy dictionary as a callable object\"\"\"\n",
        "\n",
        "  def callable_policy(observation):\n",
        "      return policy_dict[tuple(observation.tolist())]\n",
        "\n",
        "  return callable_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srQzHwZTe_gd"
      },
      "source": [
        "### Explicit model\n",
        "\n",
        "The algorithms we will implement require that a complete model of the environment is available, in the form of explicit transition and reward functions. This is not part of the gym interface.\n",
        "\n",
        "These two functions are stored in two members:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DczwaYuUfqHg"
      },
      "outputs": [],
      "source": [
        "# Transition and reward functions\n",
        "T = env.T\n",
        "R = env.R\n",
        "\n",
        "# These are represented as dictionaries (for maximum clarity)\n",
        "#   and indexed as T[state][action][next_state]\n",
        "print(\"A few probabilities\")\n",
        "print(T[(1, 1, 0)][2][(2, 1, 0)])\n",
        "print(T[(1, 1, 0)][2][(1, 1, 1)])\n",
        "print(T[(3, 1, 0)][1][(3, 1, 1)])\n",
        "\n",
        "#   also R[state][action]\n",
        "print(\"\\nA few rewards\")\n",
        "print(R[(1, 1, 0)][2])\n",
        "print(R[(3, 3, 0)][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL1bEcGG-9zc"
      },
      "outputs": [],
      "source": [
        "#env.pretty_print_T()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQglr5MOiDCp"
      },
      "outputs": [],
      "source": [
        "# Explicit set of states and actions\n",
        "\n",
        "print(\"States\", env.states)\n",
        "print(\"Actions\", env.actions)\n",
        "print(\"Number of states\", len(env.states))\n",
        "print(\"Number of actions\", len(env.actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfYfq9HebRHw"
      },
      "outputs": [],
      "source": [
        "# Some classic imports for the rest of the notebook\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import math\n",
        "\n",
        "mpl.style.use('seaborn-v0_8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L7zlYG8WcPq"
      },
      "source": [
        "## Solving the Minigrid task\n",
        "\n",
        "We would like to find a **stationary deterministic memoryless policy** or mapping from states to actions $\\pi : \\mathcal{S}\\rightarrow \\mathcal{A}$, that is able to arrive at the **goal state** sooner.\n",
        "\n",
        "In other words, our objective is to find $\\pi$ which maximizes the **expected discounted return** from any starting state represented by:\n",
        "\n",
        "\\begin{align*}\n",
        "    \\rho(\\pi) &= \\mathbb{E}_{s_0\\sim\\nu_0, s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\right]\\\\\n",
        "    &= \\sum_{s}\\nu_0(s) V^{\\pi}(s)\n",
        "\\end{align*}\n",
        "\n",
        "with the **discount factor** $\\gamma\\in[0,1)$.\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "---\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Recall that the **state value function** for $s\\in \\mathcal{S}$ denoted\n",
        "\n",
        "\\begin{align*}\n",
        "    V^\\pi(s) &= \\mathbb{E}_{s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\bigg|s_0=s\\right]\\\\\n",
        "    &=  \\sum_{a}\\pi(a|s)Q^{\\pi}(s,a)\n",
        "\\end{align*}\n",
        "\n",
        "represents the value of being in state $s$ and following policy $\\pi$ while the **action-value function** for $s\\in \\mathcal{S}, a\\in \\mathcal{A}$ denoted\n",
        "\n",
        "$$ Q^\\pi(s,a) = r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')$$\n",
        "\n",
        "is the value of first taking an action $a$ in state $s$ then following policy $\\pi$.\n",
        "\n",
        "By standard notation, the **Bellman operator** of policy $\\pi$ acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
        "\n",
        "$$(T^{\\pi}V)(s) =  \\sum_{a}\\pi(a|s)\\bigg[r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg],\\quad s\\in\\mathcal{S}$$\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "---\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "Ideally, we would like to find an **optimal policy** $\\pi^*$ with:\n",
        "\n",
        "$$ \\pi^*(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi^*}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
        "\n",
        "that maximizes our immediate reward and future return.\n",
        "\n",
        "The Bellman operator of $\\pi^*$ (a.k.a the **Bellman optimality operator**) acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
        "\n",
        "$$(T^{*}V)(s) =  \\max_{a}\\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "---\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "With the transition and reward function of the grid-world task available to us, we attempt to find an optimal policy via **Dynamic programming**. Precisely, we implement **Policy Iteration** and **Value iteration** methods introduced in the first lecture.\n",
        "\n",
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWFCbNN7F8l"
      },
      "source": [
        "### Notation\n",
        "\n",
        "*   $r_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|}$ so that for $s\\in\\mathcal{S}$, $r_{\\pi}(s) = r(s,\\pi(s))$.\n",
        "*   $p_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}|}$ so that for $s,s'\\in\\mathcal{S}$, $p_{\\pi}(s'|s) = p(s'|s,\\pi(s))$.\n",
        "*   $\\sum_{r}p(s',r|s,a)  = p(s'|s,a)r(s,a)$ since rewards are deterministic.\n",
        "*   $n_{k}$ is the number of loops required to compute $V_{k}$\n",
        "*   $\\Delta$ is a threshold on the accuracy of value estimation [[3]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1)\n",
        "\n",
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nljnBe4bZTf"
      },
      "source": [
        "### Policy Iteration (PI) Recap\n",
        "\n",
        "**Idea**\n",
        "\n",
        "Gradually advance to $\\pi^{*}$ from an initial guess $\\pi_0$ through a series of **policy evaluation** and **policy improvement** steps.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Policy Evaluation step**\n",
        "\n",
        "Given a policy $\\pi_k$, compute $V^{\\pi_k}$ as\n",
        "\n",
        "* $V^{\\pi_k} = (I - \\gamma\\, p_{\\pi_k})^{-1}r_{\\pi_k}$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Policy Improvement step**\n",
        "\n",
        "Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$ (or $V_{k}$). That is,\n",
        "$$ \\pi_{k+1}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V^{\\pi_{k}}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "#### Putting everything together\n",
        "\n",
        "Starting from an arbitrary stationary deterministic markovian policy $\\pi_{0}$, for $k = 0,1,2,\\cdots, K$ do:\n",
        "* Compute $V^{\\pi_k}$\n",
        "* Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$\n",
        "* Terminate loop if policy is stable (i.e $\\pi_{k+1}(s) = \\pi_{k}(s)$ for all $s\\in\\mathcal{S}$) and return $\\pi_{k+1}$.\n",
        "\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "#### Theoretical guarantee [[1,2]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1)\n",
        "\n",
        "PI finds an optimal policy after $K = \\mathcal{\\tilde{O}}(\\mathcal{SA}/(1-\\gamma))$ iterations.\n",
        "\n",
        "<!---\n",
        "Explicitly $K = \\mathcal{\\tilde{O}}((\\mathcal{SA - S})/(1-\\gamma))$ iterations.\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFYLOHtPKwsG"
      },
      "outputs": [],
      "source": [
        "# Don't look at the solutions! This module contains the functions you should write\n",
        "from rlss_practice import dp_solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Woh49RAfbieB"
      },
      "outputs": [],
      "source": [
        "class PolicyIteration:\n",
        "  \"\"\"\n",
        "  Implements policy iteration\n",
        "  \"\"\"\n",
        "  def __init__(self, env: Env, gamma: float, initial_policy = None):\n",
        "    # Store\n",
        "    self.env = env\n",
        "    self.states = self.env.states\n",
        "    self.n_states = len(self.states)\n",
        "    self.actions = self.env.actions\n",
        "    self.n_actions = len(self.actions)\n",
        "    self.gamma = gamma\n",
        "    self.policy = initial_policy\n",
        "\n",
        "    # Default policy\n",
        "    if self.policy == None:\n",
        "      np.random.seed(4)\n",
        "      self.policy = {state: np.random.randint(0, self.n_actions-1) for state in self.states}\n",
        "\n",
        "    self.policy_stable = False\n",
        "    self.V = {state: 0.0 for state in env.states}\n",
        "    self.V_logs = []\n",
        "\n",
        "  def _evaluate_policy(self):\n",
        "    \"\"\"\n",
        "    Given 'policy' π_{k} compute V^{π_{k}}. Let,\n",
        "\n",
        "      A = (I - \\gamma p_{π_{k}})\n",
        "      b = r_{π_{k}}\n",
        "      x = V^{π_{k}}\n",
        "\n",
        "    solve the system of linear equations Ax=b\n",
        "\n",
        "    :return x: a |S|x1 array\n",
        "    \"\"\"\n",
        "    # Task: complete this and return x instead of using the line \"return dp_solutions...\"\n",
        "\n",
        "    # TODO: Compute A\n",
        "    # TODO: Compute b\n",
        "    # TODO: solve for x\n",
        "\n",
        "    return dp_solutions.PolicyIteration._evaluate_policy(self)\n",
        "\n",
        "\n",
        "  def evaluate_policy(self):\n",
        "    \"\"\"\n",
        "    Collect state values in dict\n",
        "    \"\"\"\n",
        "    V_array = self._evaluate_policy()\n",
        "\n",
        "    # Assign values to holder as dictionary\n",
        "    self.V = {state: V_array[i].item() for i, state in enumerate(self.states)}\n",
        "\n",
        "    # Append values to log for plots\n",
        "    self.V_logs.append(self.V.copy())\n",
        "\n",
        "\n",
        "  def get_policy(self):\n",
        "    \"\"\"\n",
        "    Get the greedy policy:\n",
        "\n",
        "      π_{k+1}(s) = argmax_{a\\in A} Q^{\\pi_{k}}(s,a)\n",
        "\n",
        "    where\n",
        "\n",
        "      Q^{\\pi_{k}}(s,a) = r(s,a) + gamma * <P(.|s,a),V^{\\pi_{k}}>\n",
        "\n",
        "    assign new policy to self.policy\n",
        "    update self.policy_stable\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Solve this and delete the next line\n",
        "    dp_solutions.PolicyIteration.get_policy(self)\n",
        "\n",
        "\n",
        "  def get_expected_update(self, state, action):\n",
        "    \"\"\"\n",
        "    Compute:\n",
        "\n",
        "      Q(s,a) = r(s,a) + gamma <p(.|s,a),V>\n",
        "\n",
        "    :param state\n",
        "    :param action\n",
        "\n",
        "    :return Q(s,a): float\n",
        "    \"\"\"\n",
        "    # TODO: Solve this and return Q(s,a) instead\n",
        "    return dp_solutions.PolicyIteration.get_expected_update(self, state, action)\n",
        "\n",
        "\n",
        "  def get_p_pi(self):\n",
        "    \"\"\"\n",
        "    Given π_{k}, compute p_{π_{k}}\n",
        "    \"\"\"\n",
        "    p_pi = np.zeros((self.n_states,self.n_states))\n",
        "\n",
        "    for s_index, s in enumerate(self.states):\n",
        "      for snext_index, snext in enumerate(self.states):\n",
        "        p_pi[s_index, snext_index] = self.env.T[s][self.policy[s]][snext]\n",
        "\n",
        "    return p_pi\n",
        "\n",
        "\n",
        "  def get_r_pi(self):\n",
        "    \"\"\"\n",
        "    Given π_{k}, compute r_{π_{k}}\n",
        "    \"\"\"\n",
        "    r_pi = np.zeros((self.n_states,1))\n",
        "\n",
        "    for i, state in enumerate(self.states):\n",
        "      r_pi[i][0] = self.env.R[state][self.policy[state]]\n",
        "\n",
        "    return r_pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pTZRo-jxBxQ"
      },
      "source": [
        "Let's try it out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKL72UxrxFJl"
      },
      "outputs": [],
      "source": [
        "# Run policy iteration\n",
        "PI_planner1 = PolicyIteration(env, gamma=0.9)\n",
        "\n",
        "while not PI_planner1.policy_stable:\n",
        "  PI_planner1.evaluate_policy()\n",
        "  PI_planner1.get_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQkER_MsJiCK"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6btXwfrRJev2"
      },
      "outputs": [],
      "source": [
        "def visualize(vlogs, state, grid_size):\n",
        "  grid_size -= 2   # walls\n",
        "\n",
        "  # Initial state values\n",
        "  fig1 = plt.figure(figsize=(4, 3))\n",
        "  ax1 = fig1.subplots()\n",
        "  ax1.set_ylabel(\"value of (1,1,0)\")\n",
        "  ax1.set_xlabel(\"iterations\")\n",
        "  initial_state_values = [value[(1, 1, 0)] for value in PI_planner1.V_logs]\n",
        "  ax1.plot(range(len(initial_state_values)), initial_state_values)\n",
        "\n",
        "  # States values over time\n",
        "  some_values_over_time = [\n",
        "    {(x, y): values[(x, y, o)] for (x, y, o) in env.states if o == 0}  # o is a fixed agent orientation\n",
        "    for values in vlogs\n",
        "  ]\n",
        "  values_over_time = [\n",
        "    np.array([[values[(x+1, y+1)] for x in range(grid_size)] for y in range(grid_size)])\n",
        "    for values in some_values_over_time\n",
        "  ]\n",
        "  vmids = [(values.max() + values.min()) / 2 for values in values_over_time]\n",
        "  vmin = min([values.min() for values in values_over_time])\n",
        "  vmax = max([values.max() for values in values_over_time])\n",
        "\n",
        "  steps = len(values_over_time)\n",
        "  fig2 = plt.figure(figsize=(4 * steps, 3))\n",
        "  axs2 = fig2.subplots(1, steps)\n",
        "  if isinstance(axs2, plt.Axes):\n",
        "    axs2 = [axs2]\n",
        "    multistep = False\n",
        "  else:\n",
        "    multistep = True\n",
        "\n",
        "  for i, ax in enumerate(axs2):\n",
        "    ax.set_title(f\"values at step {i}\" if multistep else \"values\")\n",
        "    ax.imshow(values_over_time[i], cmap=\"Blues\", vmin=vmin, vmax=vmax)\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    for x in range(grid_size):\n",
        "      for y in range(grid_size):\n",
        "        val = values_over_time[i][y,x]\n",
        "        ax.text(x, y, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=\"w\" if val > vmids[i] else \"k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvCmq5bQQqEi"
      },
      "outputs": [],
      "source": [
        "visualize(PI_planner1.V_logs, (1, 1, 0), 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meYQb9VOTnDT"
      },
      "source": [
        "We can also test the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C-WmCiVT6op"
      },
      "outputs": [],
      "source": [
        "avg_return = rollouts(env=visible_env,\n",
        "                      policy=make_policy(PI_planner1.policy),\n",
        "                      n_trajectories=20,\n",
        "                      gamma=0.9)\n",
        "\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGxMz5yZEZ-N"
      },
      "outputs": [],
      "source": [
        "visible_env.play()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymYjjFBYbiy-"
      },
      "source": [
        "What if we have failure probabilities now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EcH0mS2bt1N"
      },
      "outputs": [],
      "source": [
        "# Define\n",
        "env = Room(\n",
        "    failure=0.2,\n",
        "    agent_start_pos=(1, 1),\n",
        "    agent_start_dir=1,\n",
        "    size=6,\n",
        ")\n",
        "visible_env = Renderer(env)\n",
        "print(env)\n",
        "\n",
        "# Plan\n",
        "PI_planner1 = PolicyIteration(env, gamma=0.9)\n",
        "\n",
        "while not PI_planner1.policy_stable:\n",
        "  PI_planner1.evaluate_policy()\n",
        "  PI_planner1.get_policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvuqeY88cCW2"
      },
      "outputs": [],
      "source": [
        "# Visualize\n",
        "avg_return = rollouts(\n",
        "    env=visible_env,\n",
        "    policy=make_policy(PI_planner1.policy),\n",
        "    n_trajectories=80,\n",
        "    gamma=0.9\n",
        ")\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-hHn_KXcKaS"
      },
      "outputs": [],
      "source": [
        "visible_env.play()\n",
        "\n",
        "visualize(PI_planner1.V_logs, (1, 1, 1), 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzJ9fQ8ceZIB"
      },
      "source": [
        "But matrix inversions can be quite expensive. Consider iterative policy evaluation instead.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "### Modified Policy Iteration (MPI) Recap\n",
        "\n",
        "**Idea**\n",
        "\n",
        "Same as PI but with **truncated policy evaluation**\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Truncated Policy Evaluation step**\n",
        "\n",
        "Given a policy $\\pi_k$, estimate $V^{\\pi_k}$ as $V_{k}$ with the following iterations\n",
        "\n",
        "  * Initialize $V$ as $\\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise, let $\\pi = \\pi_k$\n",
        "\n",
        "    <div>\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=1QaMg7a6HELjYycAnm6RE9vnzHD_fnaCn\" alt=\"iterative policy evaluation\" width=\"600\"/>\n",
        "    </div>\n",
        "\n",
        "    Return $V_k = V$. <!--$(T^{\\pi_{k}})^{n_{k}}V_{k-1}$-->\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "#### Putting everything together\n",
        "\n",
        "Starting from an arbitrary stationary deterministic markovian policy $\\pi_{0}$, for $k = 0,1,2,\\cdots, K$ do:\n",
        "* Estimate $V^{\\pi_k}$ with $V_{k}=(T^{\\pi_{k}})^{n_{k}}V_{k-1}$\n",
        "* Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$\n",
        "* Terminate loop if policy is stable (i.e $\\pi_{k+1}(s) = \\pi_{k}(s)$ for all $s\\in\\mathcal{S}$) and return $\\pi_{k+1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFkMVMI3syvt"
      },
      "outputs": [],
      "source": [
        "class ModifiedPolicyIteration(PolicyIteration):\n",
        "  \"\"\"\n",
        "  Implements policy iteration with truncated policy evaluation\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "              env: Env,\n",
        "              gamma: float,\n",
        "              theta: float,\n",
        "              initial_policy = None):\n",
        "\n",
        "    super().__init__(env, gamma, initial_policy)\n",
        "    self.theta = theta\n",
        "    self.V = {state: 0.0 for state in env.states}\n",
        "\n",
        "\n",
        "  def evaluate_policy(self):\n",
        "    \"\"\"\n",
        "    Given 'policy' π_{k},\n",
        "    Starting from a previous guess 'V_{k-1}',\n",
        "    iteratively estimate V^{π_{k}} as T^{π_{k}}V_{k-1}.\n",
        "\n",
        "    assign new state values to self.V\n",
        "    \"\"\"\n",
        "    # Task: Complete this and delete line *\n",
        "\n",
        "    # Set Delta\n",
        "    delta = np.inf\n",
        "\n",
        "    # Main loop for iterative value update\n",
        "    while delta > self.theta:\n",
        "      # Initialize Delta\n",
        "      delta = 0\n",
        "\n",
        "      # ToDo: Loop over states\n",
        "\n",
        "        #ToDo: Update the state value\n",
        "\n",
        "        #ToDo: Update Delta\n",
        "\n",
        "    dp_solutions.ModifiedPolicyIteration._evaluate_policy(self)    #*\n",
        "\n",
        "    self.V_logs.append(self.V.copy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KofhGOYGhckf"
      },
      "source": [
        "Now we run Modified policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9uBUwD-hhSF"
      },
      "outputs": [],
      "source": [
        "MPI_planner = ModifiedPolicyIteration(env,\n",
        "                                      gamma = 0.9, # Same as PI\n",
        "                                      theta = 0.01)\n",
        "\n",
        "while not MPI_planner.policy_stable:\n",
        "  MPI_planner.evaluate_policy()\n",
        "  MPI_planner.get_policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70ieiSAZqR6H"
      },
      "outputs": [],
      "source": [
        "avg_return = rollouts(env=visible_env,\n",
        "                      policy=make_policy(MPI_planner.policy),\n",
        "                      n_trajectories=80,\n",
        "                      gamma=0.9)\n",
        "\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCueLRiiqvaQ"
      },
      "outputs": [],
      "source": [
        "visible_env.play()\n",
        "\n",
        "visualize(MPI_planner.V_logs, (1, 1, 1), 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA8AkX2Feuwt"
      },
      "source": [
        "### Value Iteration (VI) Recap\n",
        "\n",
        "**Idea**\n",
        "\n",
        "Gradually advance to $\\pi^{*}$ with combined **truncated policy evaluation** and **policy improvement** steps.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "**Truncated Policy Evaluation and Improvement step**\n",
        "\n",
        "No arbitrary starting policy needed. Estimate $V^{\\pi^*}$ as follows:\n",
        "\n",
        "  * For $k = 0,1,2,\\cdots, K$\n",
        "\n",
        "    * Set $V = \\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise\n",
        "      <div>\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1fsi3ZZgqZ-p061AxvSdeluWZTlwSjnGj\" alt=\"policy evaluation and improvement\" width=\"400\"/>\n",
        "      </div>\n",
        "\n",
        "      return $V$ as $V_{k}$.\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "#### Putting everything together\n",
        "\n",
        "* Estimate $V^{\\pi^*}$ with $V_{K} = (T^*)^{K}\\mathbf{0}$\n",
        "* Return $\\hat{\\pi}^{*}$ as the greedy policy w.r.t $V_{K}$. That is,\n",
        "\n",
        "  $$ \\hat{\\pi}^{*}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V_{K}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
        "\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "#### Theoretical guarantee [[2]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1)\n",
        "\n",
        "VI finds an **$\\mathbf{\\varepsilon}$-optimal policy** ($\\pi^{\\varepsilon}$) satisfying\n",
        "\n",
        "$\\qquad V^* - V^{\\pi^{\\varepsilon}}\\leq \\varepsilon\\,\\mathbf{1}$\n",
        "\n",
        "after $K = \\mathcal{O}(\\ln(2\\gamma/\\varepsilon(1-\\gamma)^2)/(1-\\gamma))$ iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWtqSTiD9Kko"
      },
      "outputs": [],
      "source": [
        "class ValueIteration:\n",
        "  \"\"\"\n",
        "  Implements value iteration\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "              env: Env,\n",
        "              gamma: float,\n",
        "              epsilon: float,\n",
        "              num_iterations = None,\n",
        "  ):\n",
        "    self.env = env\n",
        "    self.states = self.env.states\n",
        "    self.n_states = len(self.states)\n",
        "    self.actions = self.env.actions\n",
        "    self.n_actions = len(self.actions)\n",
        "    self.gamma = gamma\n",
        "    self.K = num_iterations\n",
        "    self.V_logs = []\n",
        "\n",
        "    if self.K is None:\n",
        "      self.K = math.ceil(np.log((2*self.gamma)/(epsilon*(1-self.gamma)**2))/(1-self.gamma))\n",
        "\n",
        "    self.policy = {state: 0 for state in self.env.states}\n",
        "    self.V = {state: 0 for state in self.env.states}\n",
        "\n",
        "\n",
        "  def estimate_vstar(self):\n",
        "    \"\"\"\n",
        "    Starting from an initial guess 'V',\n",
        "    iteratively estimate V^{π*} as T*V\n",
        "\n",
        "    Assign new values to self.V\n",
        "    \"\"\"\n",
        "    # Task: Complete this and delete line *\n",
        "\n",
        "    # Main loop for iterative value update\n",
        "    for k in range(self.K):\n",
        "\n",
        "      # ToDo: Loop over states\n",
        "\n",
        "      # ToDo: Update the state value\n",
        "      pass\n",
        "\n",
        "\n",
        "    dp_solutions.ValueIteration._estimate_vstar(self)    #*\n",
        "\n",
        "    self.V_logs.append(self.V.copy())\n",
        "\n",
        "\n",
        "  def get_policy(self):\n",
        "    \"\"\"\n",
        "    Get the greedy policy:\n",
        "      π(s) = argmax_{a\\in A} Q(s,a)\n",
        "    where\n",
        "      Q(s,a) = r(s,a) + gamma*<P(.|s,a),v>\n",
        "\n",
        "    assign new policy to self.policy\n",
        "    \"\"\"\n",
        "    # TODO: Solve this and delete the next line\n",
        "    return PolicyIteration.get_policy(self)\n",
        "\n",
        "\n",
        "  def get_expected_update(self, state, action):\n",
        "    \"\"\"\n",
        "    Compute:\n",
        "\n",
        "      Q(s,a) = r(s,a) + gamma <p(.|s,a),V>\n",
        "\n",
        "    :param state\n",
        "    :param action\n",
        "\n",
        "    :return Q(s,a): float\n",
        "    \"\"\"\n",
        "    # TODO: Same as for PI. Solve this and return Q(s,a) instead\n",
        "    return PolicyIteration.get_expected_update(self, state, action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7DXuWm3v82W"
      },
      "source": [
        "Finally, we also plan with Value Iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFFnzxCfaS-E"
      },
      "outputs": [],
      "source": [
        "VI_planner = ValueIteration(env,\n",
        "                            gamma = 0.9, #same as PI\n",
        "                            epsilon = 0.01,\n",
        "                            num_iterations = None) # You can set the number of iterations\n",
        "\n",
        "VI_planner.estimate_vstar()\n",
        "VI_planner.get_policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgj9qrYohLWm"
      },
      "outputs": [],
      "source": [
        "avg_return = rollouts(env=visible_env,\n",
        "                      policy=make_policy(VI_planner.policy),\n",
        "                      n_trajectories=80,\n",
        "                      gamma=0.9)\n",
        "\n",
        "print(avg_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY-Vrp5J0gDC"
      },
      "outputs": [],
      "source": [
        "visible_env.play()\n",
        "\n",
        "visualize(VI_planner.V_logs, (1, 1, 0), 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fS7zGk2tas0"
      },
      "source": [
        "### Bonus Task ⭐\n",
        "\n",
        "Implement Optimistic Policy Iteration. See slide 33 of [[1]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1).\n",
        "\n",
        "Improve the efficiency of the implementations above with Numpy/Pytorch/Tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqJPFe6f6gFV"
      },
      "source": [
        "# Credit:\n",
        "\n",
        "1.   Bruno Scherrer, \"a lecture on Markov Decision Processes and Dynamic Programming\", June 2023, [Slides](https://drive.google.com/file/d/1sFh0TyU_nq60R7kouPg6MCI4IMYicSII/view?usp=sharing)\n",
        "2.   Csaba Szepesvári \"a lecture series on Theoretical Foundations of Reinforcement Learning\", 2020, [RL Theory course](https://rltheory.github.io/)\n",
        "3.   Richard S. Sutton, Andrew G. Barto \"Reinforcement Learning: An Introduction\", second edition, 2020, [Book](http://incompleteideas.net/book/RLbook2020.pdf)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}