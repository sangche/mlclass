{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangche/mlclass/blob/master/dql-cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chatGPT code"
      ],
      "metadata": {
        "id": "sIwT-0CHu1Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "# Define the Q-Network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Replay Buffer for Experience Replay\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Train the DQN agent\n",
        "def train_dqn(env, episodes=500, gamma=0.99, lr=0.001, batch_size=64, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy_net = DQN(state_dim, action_dim).to(device)\n",
        "    target_net = DQN(state_dim, action_dim).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "    replay_buffer = ReplayBuffer(10000)\n",
        "    epsilon = 1.0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Epsilon-greedy action selection\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    action = policy_net(state).argmax().item()\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            replay_buffer.push(state.cpu().numpy(), action, reward, next_state.cpu().numpy(), done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Train the network if enough samples exist\n",
        "            if len(replay_buffer) > batch_size:\n",
        "                batch = replay_buffer.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "                states = torch.FloatTensor(states).to(device)\n",
        "                actions = torch.LongTensor(actions).to(device)\n",
        "                rewards = torch.FloatTensor(rewards).to(device)\n",
        "                next_states = torch.FloatTensor(next_states).to(device)\n",
        "                dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "                q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "                next_q_values = target_net(next_states).max(1)[0]\n",
        "                target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "                loss = F.mse_loss(q_values, target_q_values.detach())\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Update epsilon\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "        # Update target network every few episodes\n",
        "        if episode % 10 == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return policy_net\n",
        "\n",
        "# Set up environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "trained_model = train_dqn(env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsyL7QSZu1qR",
        "outputId": "812ac4fe-9886-48f8-92b9-c298d4349377"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-1-043d81e7439d>:74: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  states = torch.FloatTensor(states).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 18.0, Epsilon: 0.9950\n",
            "Episode 1, Total Reward: 33.0, Epsilon: 0.9900\n",
            "Episode 2, Total Reward: 14.0, Epsilon: 0.9851\n",
            "Episode 3, Total Reward: 16.0, Epsilon: 0.9801\n",
            "Episode 4, Total Reward: 18.0, Epsilon: 0.9752\n",
            "Episode 5, Total Reward: 12.0, Epsilon: 0.9704\n",
            "Episode 6, Total Reward: 23.0, Epsilon: 0.9655\n",
            "Episode 7, Total Reward: 23.0, Epsilon: 0.9607\n",
            "Episode 8, Total Reward: 19.0, Epsilon: 0.9559\n",
            "Episode 9, Total Reward: 24.0, Epsilon: 0.9511\n",
            "Episode 10, Total Reward: 14.0, Epsilon: 0.9464\n",
            "Episode 11, Total Reward: 18.0, Epsilon: 0.9416\n",
            "Episode 12, Total Reward: 40.0, Epsilon: 0.9369\n",
            "Episode 13, Total Reward: 17.0, Epsilon: 0.9322\n",
            "Episode 14, Total Reward: 15.0, Epsilon: 0.9276\n",
            "Episode 15, Total Reward: 10.0, Epsilon: 0.9229\n",
            "Episode 16, Total Reward: 16.0, Epsilon: 0.9183\n",
            "Episode 17, Total Reward: 15.0, Epsilon: 0.9137\n",
            "Episode 18, Total Reward: 22.0, Epsilon: 0.9092\n",
            "Episode 19, Total Reward: 15.0, Epsilon: 0.9046\n",
            "Episode 20, Total Reward: 23.0, Epsilon: 0.9001\n",
            "Episode 21, Total Reward: 23.0, Epsilon: 0.8956\n",
            "Episode 22, Total Reward: 15.0, Epsilon: 0.8911\n",
            "Episode 23, Total Reward: 47.0, Epsilon: 0.8867\n",
            "Episode 24, Total Reward: 26.0, Epsilon: 0.8822\n",
            "Episode 25, Total Reward: 27.0, Epsilon: 0.8778\n",
            "Episode 26, Total Reward: 11.0, Epsilon: 0.8734\n",
            "Episode 27, Total Reward: 15.0, Epsilon: 0.8691\n",
            "Episode 28, Total Reward: 76.0, Epsilon: 0.8647\n",
            "Episode 29, Total Reward: 25.0, Epsilon: 0.8604\n",
            "Episode 30, Total Reward: 16.0, Epsilon: 0.8561\n",
            "Episode 31, Total Reward: 31.0, Epsilon: 0.8518\n",
            "Episode 32, Total Reward: 24.0, Epsilon: 0.8475\n",
            "Episode 33, Total Reward: 30.0, Epsilon: 0.8433\n",
            "Episode 34, Total Reward: 34.0, Epsilon: 0.8391\n",
            "Episode 35, Total Reward: 20.0, Epsilon: 0.8349\n",
            "Episode 36, Total Reward: 10.0, Epsilon: 0.8307\n",
            "Episode 37, Total Reward: 12.0, Epsilon: 0.8266\n",
            "Episode 38, Total Reward: 30.0, Epsilon: 0.8224\n",
            "Episode 39, Total Reward: 69.0, Epsilon: 0.8183\n",
            "Episode 40, Total Reward: 22.0, Epsilon: 0.8142\n",
            "Episode 41, Total Reward: 20.0, Epsilon: 0.8102\n",
            "Episode 42, Total Reward: 29.0, Epsilon: 0.8061\n",
            "Episode 43, Total Reward: 15.0, Epsilon: 0.8021\n",
            "Episode 44, Total Reward: 48.0, Epsilon: 0.7981\n",
            "Episode 45, Total Reward: 18.0, Epsilon: 0.7941\n",
            "Episode 46, Total Reward: 64.0, Epsilon: 0.7901\n",
            "Episode 47, Total Reward: 17.0, Epsilon: 0.7862\n",
            "Episode 48, Total Reward: 20.0, Epsilon: 0.7822\n",
            "Episode 49, Total Reward: 18.0, Epsilon: 0.7783\n",
            "Episode 50, Total Reward: 48.0, Epsilon: 0.7744\n",
            "Episode 51, Total Reward: 22.0, Epsilon: 0.7705\n",
            "Episode 52, Total Reward: 33.0, Epsilon: 0.7667\n",
            "Episode 53, Total Reward: 31.0, Epsilon: 0.7629\n",
            "Episode 54, Total Reward: 18.0, Epsilon: 0.7590\n",
            "Episode 55, Total Reward: 24.0, Epsilon: 0.7553\n",
            "Episode 56, Total Reward: 15.0, Epsilon: 0.7515\n",
            "Episode 57, Total Reward: 15.0, Epsilon: 0.7477\n",
            "Episode 58, Total Reward: 11.0, Epsilon: 0.7440\n",
            "Episode 59, Total Reward: 73.0, Epsilon: 0.7403\n",
            "Episode 60, Total Reward: 65.0, Epsilon: 0.7366\n",
            "Episode 61, Total Reward: 59.0, Epsilon: 0.7329\n",
            "Episode 62, Total Reward: 12.0, Epsilon: 0.7292\n",
            "Episode 63, Total Reward: 31.0, Epsilon: 0.7256\n",
            "Episode 64, Total Reward: 25.0, Epsilon: 0.7219\n",
            "Episode 65, Total Reward: 31.0, Epsilon: 0.7183\n",
            "Episode 66, Total Reward: 92.0, Epsilon: 0.7147\n",
            "Episode 67, Total Reward: 16.0, Epsilon: 0.7112\n",
            "Episode 68, Total Reward: 28.0, Epsilon: 0.7076\n",
            "Episode 69, Total Reward: 17.0, Epsilon: 0.7041\n",
            "Episode 70, Total Reward: 38.0, Epsilon: 0.7005\n",
            "Episode 71, Total Reward: 17.0, Epsilon: 0.6970\n",
            "Episode 72, Total Reward: 15.0, Epsilon: 0.6936\n",
            "Episode 73, Total Reward: 22.0, Epsilon: 0.6901\n",
            "Episode 74, Total Reward: 36.0, Epsilon: 0.6866\n",
            "Episode 75, Total Reward: 26.0, Epsilon: 0.6832\n",
            "Episode 76, Total Reward: 72.0, Epsilon: 0.6798\n",
            "Episode 77, Total Reward: 103.0, Epsilon: 0.6764\n",
            "Episode 78, Total Reward: 60.0, Epsilon: 0.6730\n",
            "Episode 79, Total Reward: 13.0, Epsilon: 0.6696\n",
            "Episode 80, Total Reward: 46.0, Epsilon: 0.6663\n",
            "Episode 81, Total Reward: 57.0, Epsilon: 0.6630\n",
            "Episode 82, Total Reward: 91.0, Epsilon: 0.6597\n",
            "Episode 83, Total Reward: 18.0, Epsilon: 0.6564\n",
            "Episode 84, Total Reward: 19.0, Epsilon: 0.6531\n",
            "Episode 85, Total Reward: 15.0, Epsilon: 0.6498\n",
            "Episode 86, Total Reward: 26.0, Epsilon: 0.6466\n",
            "Episode 87, Total Reward: 33.0, Epsilon: 0.6433\n",
            "Episode 88, Total Reward: 32.0, Epsilon: 0.6401\n",
            "Episode 89, Total Reward: 56.0, Epsilon: 0.6369\n",
            "Episode 90, Total Reward: 98.0, Epsilon: 0.6337\n",
            "Episode 91, Total Reward: 30.0, Epsilon: 0.6306\n",
            "Episode 92, Total Reward: 21.0, Epsilon: 0.6274\n",
            "Episode 93, Total Reward: 96.0, Epsilon: 0.6243\n",
            "Episode 94, Total Reward: 39.0, Epsilon: 0.6211\n",
            "Episode 95, Total Reward: 117.0, Epsilon: 0.6180\n",
            "Episode 96, Total Reward: 46.0, Epsilon: 0.6149\n",
            "Episode 97, Total Reward: 42.0, Epsilon: 0.6119\n",
            "Episode 98, Total Reward: 36.0, Epsilon: 0.6088\n",
            "Episode 99, Total Reward: 66.0, Epsilon: 0.6058\n",
            "Episode 100, Total Reward: 33.0, Epsilon: 0.6027\n",
            "Episode 101, Total Reward: 132.0, Epsilon: 0.5997\n",
            "Episode 102, Total Reward: 79.0, Epsilon: 0.5967\n",
            "Episode 103, Total Reward: 48.0, Epsilon: 0.5937\n",
            "Episode 104, Total Reward: 140.0, Epsilon: 0.5908\n",
            "Episode 105, Total Reward: 32.0, Epsilon: 0.5878\n",
            "Episode 106, Total Reward: 17.0, Epsilon: 0.5849\n",
            "Episode 107, Total Reward: 66.0, Epsilon: 0.5820\n",
            "Episode 108, Total Reward: 12.0, Epsilon: 0.5790\n",
            "Episode 109, Total Reward: 132.0, Epsilon: 0.5762\n",
            "Episode 110, Total Reward: 79.0, Epsilon: 0.5733\n",
            "Episode 111, Total Reward: 38.0, Epsilon: 0.5704\n",
            "Episode 112, Total Reward: 97.0, Epsilon: 0.5676\n",
            "Episode 113, Total Reward: 25.0, Epsilon: 0.5647\n",
            "Episode 114, Total Reward: 104.0, Epsilon: 0.5619\n",
            "Episode 115, Total Reward: 72.0, Epsilon: 0.5591\n",
            "Episode 116, Total Reward: 125.0, Epsilon: 0.5563\n",
            "Episode 117, Total Reward: 103.0, Epsilon: 0.5535\n",
            "Episode 118, Total Reward: 120.0, Epsilon: 0.5507\n",
            "Episode 119, Total Reward: 98.0, Epsilon: 0.5480\n",
            "Episode 120, Total Reward: 36.0, Epsilon: 0.5452\n",
            "Episode 121, Total Reward: 32.0, Epsilon: 0.5425\n",
            "Episode 122, Total Reward: 185.0, Epsilon: 0.5398\n",
            "Episode 123, Total Reward: 88.0, Epsilon: 0.5371\n",
            "Episode 124, Total Reward: 59.0, Epsilon: 0.5344\n",
            "Episode 125, Total Reward: 34.0, Epsilon: 0.5318\n",
            "Episode 126, Total Reward: 24.0, Epsilon: 0.5291\n",
            "Episode 127, Total Reward: 90.0, Epsilon: 0.5264\n",
            "Episode 128, Total Reward: 50.0, Epsilon: 0.5238\n",
            "Episode 129, Total Reward: 115.0, Epsilon: 0.5212\n",
            "Episode 130, Total Reward: 64.0, Epsilon: 0.5186\n",
            "Episode 131, Total Reward: 11.0, Epsilon: 0.5160\n",
            "Episode 132, Total Reward: 149.0, Epsilon: 0.5134\n",
            "Episode 133, Total Reward: 47.0, Epsilon: 0.5108\n",
            "Episode 134, Total Reward: 140.0, Epsilon: 0.5083\n",
            "Episode 135, Total Reward: 72.0, Epsilon: 0.5058\n",
            "Episode 136, Total Reward: 82.0, Epsilon: 0.5032\n",
            "Episode 137, Total Reward: 21.0, Epsilon: 0.5007\n",
            "Episode 138, Total Reward: 75.0, Epsilon: 0.4982\n",
            "Episode 139, Total Reward: 101.0, Epsilon: 0.4957\n",
            "Episode 140, Total Reward: 50.0, Epsilon: 0.4932\n",
            "Episode 141, Total Reward: 21.0, Epsilon: 0.4908\n",
            "Episode 142, Total Reward: 100.0, Epsilon: 0.4883\n",
            "Episode 143, Total Reward: 50.0, Epsilon: 0.4859\n",
            "Episode 144, Total Reward: 27.0, Epsilon: 0.4834\n",
            "Episode 145, Total Reward: 113.0, Epsilon: 0.4810\n",
            "Episode 146, Total Reward: 65.0, Epsilon: 0.4786\n",
            "Episode 147, Total Reward: 43.0, Epsilon: 0.4762\n",
            "Episode 148, Total Reward: 100.0, Epsilon: 0.4738\n",
            "Episode 149, Total Reward: 170.0, Epsilon: 0.4715\n",
            "Episode 150, Total Reward: 48.0, Epsilon: 0.4691\n",
            "Episode 151, Total Reward: 15.0, Epsilon: 0.4668\n",
            "Episode 152, Total Reward: 105.0, Epsilon: 0.4644\n",
            "Episode 153, Total Reward: 82.0, Epsilon: 0.4621\n",
            "Episode 154, Total Reward: 18.0, Epsilon: 0.4598\n",
            "Episode 155, Total Reward: 24.0, Epsilon: 0.4575\n",
            "Episode 156, Total Reward: 23.0, Epsilon: 0.4552\n",
            "Episode 157, Total Reward: 140.0, Epsilon: 0.4529\n",
            "Episode 158, Total Reward: 170.0, Epsilon: 0.4507\n",
            "Episode 159, Total Reward: 44.0, Epsilon: 0.4484\n",
            "Episode 160, Total Reward: 15.0, Epsilon: 0.4462\n",
            "Episode 161, Total Reward: 110.0, Epsilon: 0.4440\n",
            "Episode 162, Total Reward: 51.0, Epsilon: 0.4417\n",
            "Episode 163, Total Reward: 138.0, Epsilon: 0.4395\n",
            "Episode 164, Total Reward: 61.0, Epsilon: 0.4373\n",
            "Episode 165, Total Reward: 65.0, Epsilon: 0.4351\n",
            "Episode 166, Total Reward: 139.0, Epsilon: 0.4330\n",
            "Episode 167, Total Reward: 30.0, Epsilon: 0.4308\n",
            "Episode 168, Total Reward: 149.0, Epsilon: 0.4286\n",
            "Episode 169, Total Reward: 30.0, Epsilon: 0.4265\n",
            "Episode 170, Total Reward: 25.0, Epsilon: 0.4244\n",
            "Episode 171, Total Reward: 57.0, Epsilon: 0.4223\n",
            "Episode 172, Total Reward: 27.0, Epsilon: 0.4201\n",
            "Episode 173, Total Reward: 45.0, Epsilon: 0.4180\n",
            "Episode 174, Total Reward: 14.0, Epsilon: 0.4159\n",
            "Episode 175, Total Reward: 19.0, Epsilon: 0.4139\n",
            "Episode 176, Total Reward: 124.0, Epsilon: 0.4118\n",
            "Episode 177, Total Reward: 20.0, Epsilon: 0.4097\n",
            "Episode 178, Total Reward: 40.0, Epsilon: 0.4077\n",
            "Episode 179, Total Reward: 13.0, Epsilon: 0.4057\n",
            "Episode 180, Total Reward: 17.0, Epsilon: 0.4036\n",
            "Episode 181, Total Reward: 16.0, Epsilon: 0.4016\n",
            "Episode 182, Total Reward: 107.0, Epsilon: 0.3996\n",
            "Episode 183, Total Reward: 127.0, Epsilon: 0.3976\n",
            "Episode 184, Total Reward: 126.0, Epsilon: 0.3956\n",
            "Episode 185, Total Reward: 23.0, Epsilon: 0.3936\n",
            "Episode 186, Total Reward: 83.0, Epsilon: 0.3917\n",
            "Episode 187, Total Reward: 84.0, Epsilon: 0.3897\n",
            "Episode 188, Total Reward: 111.0, Epsilon: 0.3878\n",
            "Episode 189, Total Reward: 106.0, Epsilon: 0.3858\n",
            "Episode 190, Total Reward: 270.0, Epsilon: 0.3839\n",
            "Episode 191, Total Reward: 88.0, Epsilon: 0.3820\n",
            "Episode 192, Total Reward: 145.0, Epsilon: 0.3801\n",
            "Episode 193, Total Reward: 13.0, Epsilon: 0.3782\n",
            "Episode 194, Total Reward: 106.0, Epsilon: 0.3763\n",
            "Episode 195, Total Reward: 102.0, Epsilon: 0.3744\n",
            "Episode 196, Total Reward: 25.0, Epsilon: 0.3725\n",
            "Episode 197, Total Reward: 15.0, Epsilon: 0.3707\n",
            "Episode 198, Total Reward: 10.0, Epsilon: 0.3688\n",
            "Episode 199, Total Reward: 15.0, Epsilon: 0.3670\n",
            "Episode 200, Total Reward: 117.0, Epsilon: 0.3651\n",
            "Episode 201, Total Reward: 143.0, Epsilon: 0.3633\n",
            "Episode 202, Total Reward: 184.0, Epsilon: 0.3615\n",
            "Episode 203, Total Reward: 63.0, Epsilon: 0.3597\n",
            "Episode 204, Total Reward: 29.0, Epsilon: 0.3579\n",
            "Episode 205, Total Reward: 16.0, Epsilon: 0.3561\n",
            "Episode 206, Total Reward: 46.0, Epsilon: 0.3543\n",
            "Episode 207, Total Reward: 21.0, Epsilon: 0.3525\n",
            "Episode 208, Total Reward: 12.0, Epsilon: 0.3508\n",
            "Episode 209, Total Reward: 73.0, Epsilon: 0.3490\n",
            "Episode 210, Total Reward: 198.0, Epsilon: 0.3473\n",
            "Episode 211, Total Reward: 43.0, Epsilon: 0.3455\n",
            "Episode 212, Total Reward: 17.0, Epsilon: 0.3438\n",
            "Episode 213, Total Reward: 42.0, Epsilon: 0.3421\n",
            "Episode 214, Total Reward: 36.0, Epsilon: 0.3404\n",
            "Episode 215, Total Reward: 26.0, Epsilon: 0.3387\n",
            "Episode 216, Total Reward: 119.0, Epsilon: 0.3370\n",
            "Episode 217, Total Reward: 117.0, Epsilon: 0.3353\n",
            "Episode 218, Total Reward: 13.0, Epsilon: 0.3336\n",
            "Episode 219, Total Reward: 95.0, Epsilon: 0.3320\n",
            "Episode 220, Total Reward: 12.0, Epsilon: 0.3303\n",
            "Episode 221, Total Reward: 66.0, Epsilon: 0.3286\n",
            "Episode 222, Total Reward: 50.0, Epsilon: 0.3270\n",
            "Episode 223, Total Reward: 16.0, Epsilon: 0.3254\n",
            "Episode 224, Total Reward: 13.0, Epsilon: 0.3237\n",
            "Episode 225, Total Reward: 70.0, Epsilon: 0.3221\n",
            "Episode 226, Total Reward: 204.0, Epsilon: 0.3205\n",
            "Episode 227, Total Reward: 226.0, Epsilon: 0.3189\n",
            "Episode 228, Total Reward: 173.0, Epsilon: 0.3173\n",
            "Episode 229, Total Reward: 34.0, Epsilon: 0.3157\n",
            "Episode 230, Total Reward: 10.0, Epsilon: 0.3141\n",
            "Episode 231, Total Reward: 28.0, Epsilon: 0.3126\n",
            "Episode 232, Total Reward: 130.0, Epsilon: 0.3110\n",
            "Episode 233, Total Reward: 16.0, Epsilon: 0.3095\n",
            "Episode 234, Total Reward: 15.0, Epsilon: 0.3079\n",
            "Episode 235, Total Reward: 41.0, Epsilon: 0.3064\n",
            "Episode 236, Total Reward: 136.0, Epsilon: 0.3048\n",
            "Episode 237, Total Reward: 116.0, Epsilon: 0.3033\n",
            "Episode 238, Total Reward: 124.0, Epsilon: 0.3018\n",
            "Episode 239, Total Reward: 114.0, Epsilon: 0.3003\n",
            "Episode 240, Total Reward: 51.0, Epsilon: 0.2988\n",
            "Episode 241, Total Reward: 39.0, Epsilon: 0.2973\n",
            "Episode 242, Total Reward: 13.0, Epsilon: 0.2958\n",
            "Episode 243, Total Reward: 161.0, Epsilon: 0.2943\n",
            "Episode 244, Total Reward: 17.0, Epsilon: 0.2929\n",
            "Episode 245, Total Reward: 35.0, Epsilon: 0.2914\n",
            "Episode 246, Total Reward: 13.0, Epsilon: 0.2899\n",
            "Episode 247, Total Reward: 192.0, Epsilon: 0.2885\n",
            "Episode 248, Total Reward: 147.0, Epsilon: 0.2870\n",
            "Episode 249, Total Reward: 187.0, Epsilon: 0.2856\n",
            "Episode 250, Total Reward: 35.0, Epsilon: 0.2842\n",
            "Episode 251, Total Reward: 13.0, Epsilon: 0.2828\n",
            "Episode 252, Total Reward: 13.0, Epsilon: 0.2813\n",
            "Episode 253, Total Reward: 468.0, Epsilon: 0.2799\n",
            "Episode 254, Total Reward: 225.0, Epsilon: 0.2785\n",
            "Episode 255, Total Reward: 34.0, Epsilon: 0.2771\n",
            "Episode 256, Total Reward: 11.0, Epsilon: 0.2758\n",
            "Episode 257, Total Reward: 176.0, Epsilon: 0.2744\n",
            "Episode 258, Total Reward: 370.0, Epsilon: 0.2730\n",
            "Episode 259, Total Reward: 149.0, Epsilon: 0.2716\n",
            "Episode 260, Total Reward: 123.0, Epsilon: 0.2703\n",
            "Episode 261, Total Reward: 11.0, Epsilon: 0.2689\n",
            "Episode 262, Total Reward: 89.0, Epsilon: 0.2676\n",
            "Episode 263, Total Reward: 104.0, Epsilon: 0.2663\n",
            "Episode 264, Total Reward: 132.0, Epsilon: 0.2649\n",
            "Episode 265, Total Reward: 182.0, Epsilon: 0.2636\n",
            "Episode 266, Total Reward: 140.0, Epsilon: 0.2623\n",
            "Episode 267, Total Reward: 12.0, Epsilon: 0.2610\n",
            "Episode 268, Total Reward: 28.0, Epsilon: 0.2597\n",
            "Episode 269, Total Reward: 277.0, Epsilon: 0.2584\n",
            "Episode 270, Total Reward: 58.0, Epsilon: 0.2571\n",
            "Episode 271, Total Reward: 152.0, Epsilon: 0.2558\n",
            "Episode 272, Total Reward: 111.0, Epsilon: 0.2545\n",
            "Episode 273, Total Reward: 233.0, Epsilon: 0.2532\n",
            "Episode 274, Total Reward: 145.0, Epsilon: 0.2520\n",
            "Episode 275, Total Reward: 270.0, Epsilon: 0.2507\n",
            "Episode 276, Total Reward: 24.0, Epsilon: 0.2495\n",
            "Episode 277, Total Reward: 500.0, Epsilon: 0.2482\n",
            "Episode 278, Total Reward: 190.0, Epsilon: 0.2470\n",
            "Episode 279, Total Reward: 198.0, Epsilon: 0.2457\n",
            "Episode 280, Total Reward: 186.0, Epsilon: 0.2445\n",
            "Episode 281, Total Reward: 204.0, Epsilon: 0.2433\n",
            "Episode 282, Total Reward: 174.0, Epsilon: 0.2421\n",
            "Episode 283, Total Reward: 220.0, Epsilon: 0.2409\n",
            "Episode 284, Total Reward: 170.0, Epsilon: 0.2397\n",
            "Episode 285, Total Reward: 160.0, Epsilon: 0.2385\n",
            "Episode 286, Total Reward: 217.0, Epsilon: 0.2373\n",
            "Episode 287, Total Reward: 190.0, Epsilon: 0.2361\n",
            "Episode 288, Total Reward: 181.0, Epsilon: 0.2349\n",
            "Episode 289, Total Reward: 178.0, Epsilon: 0.2337\n",
            "Episode 290, Total Reward: 20.0, Epsilon: 0.2326\n",
            "Episode 291, Total Reward: 265.0, Epsilon: 0.2314\n",
            "Episode 292, Total Reward: 155.0, Epsilon: 0.2302\n",
            "Episode 293, Total Reward: 180.0, Epsilon: 0.2291\n",
            "Episode 294, Total Reward: 413.0, Epsilon: 0.2279\n",
            "Episode 295, Total Reward: 33.0, Epsilon: 0.2268\n",
            "Episode 296, Total Reward: 192.0, Epsilon: 0.2257\n",
            "Episode 297, Total Reward: 162.0, Epsilon: 0.2245\n",
            "Episode 298, Total Reward: 167.0, Epsilon: 0.2234\n",
            "Episode 299, Total Reward: 103.0, Epsilon: 0.2223\n",
            "Episode 300, Total Reward: 500.0, Epsilon: 0.2212\n",
            "Episode 301, Total Reward: 160.0, Epsilon: 0.2201\n",
            "Episode 302, Total Reward: 283.0, Epsilon: 0.2190\n",
            "Episode 303, Total Reward: 54.0, Epsilon: 0.2179\n",
            "Episode 304, Total Reward: 96.0, Epsilon: 0.2168\n",
            "Episode 305, Total Reward: 267.0, Epsilon: 0.2157\n",
            "Episode 306, Total Reward: 500.0, Epsilon: 0.2146\n",
            "Episode 307, Total Reward: 287.0, Epsilon: 0.2136\n",
            "Episode 308, Total Reward: 26.0, Epsilon: 0.2125\n",
            "Episode 309, Total Reward: 216.0, Epsilon: 0.2114\n",
            "Episode 310, Total Reward: 459.0, Epsilon: 0.2104\n",
            "Episode 311, Total Reward: 124.0, Epsilon: 0.2093\n",
            "Episode 312, Total Reward: 160.0, Epsilon: 0.2083\n",
            "Episode 313, Total Reward: 262.0, Epsilon: 0.2072\n",
            "Episode 314, Total Reward: 256.0, Epsilon: 0.2062\n",
            "Episode 315, Total Reward: 332.0, Epsilon: 0.2052\n",
            "Episode 316, Total Reward: 182.0, Epsilon: 0.2041\n",
            "Episode 317, Total Reward: 500.0, Epsilon: 0.2031\n",
            "Episode 318, Total Reward: 232.0, Epsilon: 0.2021\n",
            "Episode 319, Total Reward: 45.0, Epsilon: 0.2011\n",
            "Episode 320, Total Reward: 131.0, Epsilon: 0.2001\n",
            "Episode 321, Total Reward: 500.0, Epsilon: 0.1991\n",
            "Episode 322, Total Reward: 160.0, Epsilon: 0.1981\n",
            "Episode 323, Total Reward: 248.0, Epsilon: 0.1971\n",
            "Episode 324, Total Reward: 84.0, Epsilon: 0.1961\n",
            "Episode 325, Total Reward: 267.0, Epsilon: 0.1951\n",
            "Episode 326, Total Reward: 33.0, Epsilon: 0.1942\n",
            "Episode 327, Total Reward: 444.0, Epsilon: 0.1932\n",
            "Episode 328, Total Reward: 137.0, Epsilon: 0.1922\n",
            "Episode 329, Total Reward: 348.0, Epsilon: 0.1913\n",
            "Episode 330, Total Reward: 387.0, Epsilon: 0.1903\n",
            "Episode 331, Total Reward: 59.0, Epsilon: 0.1893\n",
            "Episode 332, Total Reward: 395.0, Epsilon: 0.1884\n",
            "Episode 333, Total Reward: 133.0, Epsilon: 0.1875\n",
            "Episode 334, Total Reward: 102.0, Epsilon: 0.1865\n",
            "Episode 335, Total Reward: 436.0, Epsilon: 0.1856\n",
            "Episode 336, Total Reward: 183.0, Epsilon: 0.1847\n",
            "Episode 337, Total Reward: 22.0, Epsilon: 0.1837\n",
            "Episode 338, Total Reward: 185.0, Epsilon: 0.1828\n",
            "Episode 339, Total Reward: 486.0, Epsilon: 0.1819\n",
            "Episode 340, Total Reward: 308.0, Epsilon: 0.1810\n",
            "Episode 341, Total Reward: 301.0, Epsilon: 0.1801\n",
            "Episode 342, Total Reward: 167.0, Epsilon: 0.1792\n",
            "Episode 343, Total Reward: 83.0, Epsilon: 0.1783\n",
            "Episode 344, Total Reward: 30.0, Epsilon: 0.1774\n",
            "Episode 345, Total Reward: 43.0, Epsilon: 0.1765\n",
            "Episode 346, Total Reward: 34.0, Epsilon: 0.1756\n",
            "Episode 347, Total Reward: 14.0, Epsilon: 0.1748\n",
            "Episode 348, Total Reward: 37.0, Epsilon: 0.1739\n",
            "Episode 349, Total Reward: 19.0, Epsilon: 0.1730\n",
            "Episode 350, Total Reward: 182.0, Epsilon: 0.1721\n",
            "Episode 351, Total Reward: 21.0, Epsilon: 0.1713\n",
            "Episode 352, Total Reward: 346.0, Epsilon: 0.1704\n",
            "Episode 353, Total Reward: 38.0, Epsilon: 0.1696\n",
            "Episode 354, Total Reward: 50.0, Epsilon: 0.1687\n",
            "Episode 355, Total Reward: 183.0, Epsilon: 0.1679\n",
            "Episode 356, Total Reward: 168.0, Epsilon: 0.1670\n",
            "Episode 357, Total Reward: 262.0, Epsilon: 0.1662\n",
            "Episode 358, Total Reward: 15.0, Epsilon: 0.1654\n",
            "Episode 359, Total Reward: 20.0, Epsilon: 0.1646\n",
            "Episode 360, Total Reward: 149.0, Epsilon: 0.1637\n",
            "Episode 361, Total Reward: 153.0, Epsilon: 0.1629\n",
            "Episode 362, Total Reward: 45.0, Epsilon: 0.1621\n",
            "Episode 363, Total Reward: 16.0, Epsilon: 0.1613\n",
            "Episode 364, Total Reward: 106.0, Epsilon: 0.1605\n",
            "Episode 365, Total Reward: 50.0, Epsilon: 0.1597\n",
            "Episode 366, Total Reward: 49.0, Epsilon: 0.1589\n",
            "Episode 367, Total Reward: 172.0, Epsilon: 0.1581\n",
            "Episode 368, Total Reward: 45.0, Epsilon: 0.1573\n",
            "Episode 369, Total Reward: 17.0, Epsilon: 0.1565\n",
            "Episode 370, Total Reward: 126.0, Epsilon: 0.1557\n",
            "Episode 371, Total Reward: 151.0, Epsilon: 0.1549\n",
            "Episode 372, Total Reward: 189.0, Epsilon: 0.1542\n",
            "Episode 373, Total Reward: 58.0, Epsilon: 0.1534\n",
            "Episode 374, Total Reward: 116.0, Epsilon: 0.1526\n",
            "Episode 375, Total Reward: 199.0, Epsilon: 0.1519\n",
            "Episode 376, Total Reward: 29.0, Epsilon: 0.1511\n",
            "Episode 377, Total Reward: 154.0, Epsilon: 0.1504\n",
            "Episode 378, Total Reward: 187.0, Epsilon: 0.1496\n",
            "Episode 379, Total Reward: 125.0, Epsilon: 0.1489\n",
            "Episode 380, Total Reward: 150.0, Epsilon: 0.1481\n",
            "Episode 381, Total Reward: 113.0, Epsilon: 0.1474\n",
            "Episode 382, Total Reward: 156.0, Epsilon: 0.1466\n",
            "Episode 383, Total Reward: 24.0, Epsilon: 0.1459\n",
            "Episode 384, Total Reward: 35.0, Epsilon: 0.1452\n",
            "Episode 385, Total Reward: 18.0, Epsilon: 0.1444\n",
            "Episode 386, Total Reward: 107.0, Epsilon: 0.1437\n",
            "Episode 387, Total Reward: 145.0, Epsilon: 0.1430\n",
            "Episode 388, Total Reward: 118.0, Epsilon: 0.1423\n",
            "Episode 389, Total Reward: 179.0, Epsilon: 0.1416\n",
            "Episode 390, Total Reward: 155.0, Epsilon: 0.1409\n",
            "Episode 391, Total Reward: 139.0, Epsilon: 0.1402\n",
            "Episode 392, Total Reward: 20.0, Epsilon: 0.1395\n",
            "Episode 393, Total Reward: 128.0, Epsilon: 0.1388\n",
            "Episode 394, Total Reward: 41.0, Epsilon: 0.1381\n",
            "Episode 395, Total Reward: 164.0, Epsilon: 0.1374\n",
            "Episode 396, Total Reward: 157.0, Epsilon: 0.1367\n",
            "Episode 397, Total Reward: 206.0, Epsilon: 0.1360\n",
            "Episode 398, Total Reward: 157.0, Epsilon: 0.1353\n",
            "Episode 399, Total Reward: 78.0, Epsilon: 0.1347\n",
            "Episode 400, Total Reward: 170.0, Epsilon: 0.1340\n",
            "Episode 401, Total Reward: 115.0, Epsilon: 0.1333\n",
            "Episode 402, Total Reward: 139.0, Epsilon: 0.1326\n",
            "Episode 403, Total Reward: 161.0, Epsilon: 0.1320\n",
            "Episode 404, Total Reward: 176.0, Epsilon: 0.1313\n",
            "Episode 405, Total Reward: 160.0, Epsilon: 0.1307\n",
            "Episode 406, Total Reward: 230.0, Epsilon: 0.1300\n",
            "Episode 407, Total Reward: 24.0, Epsilon: 0.1294\n",
            "Episode 408, Total Reward: 47.0, Epsilon: 0.1287\n",
            "Episode 409, Total Reward: 275.0, Epsilon: 0.1281\n",
            "Episode 410, Total Reward: 186.0, Epsilon: 0.1274\n",
            "Episode 411, Total Reward: 156.0, Epsilon: 0.1268\n",
            "Episode 412, Total Reward: 163.0, Epsilon: 0.1262\n",
            "Episode 413, Total Reward: 234.0, Epsilon: 0.1255\n",
            "Episode 414, Total Reward: 500.0, Epsilon: 0.1249\n",
            "Episode 415, Total Reward: 178.0, Epsilon: 0.1243\n",
            "Episode 416, Total Reward: 40.0, Epsilon: 0.1237\n",
            "Episode 417, Total Reward: 126.0, Epsilon: 0.1230\n",
            "Episode 418, Total Reward: 196.0, Epsilon: 0.1224\n",
            "Episode 419, Total Reward: 500.0, Epsilon: 0.1218\n",
            "Episode 420, Total Reward: 389.0, Epsilon: 0.1212\n",
            "Episode 421, Total Reward: 500.0, Epsilon: 0.1206\n",
            "Episode 422, Total Reward: 436.0, Epsilon: 0.1200\n",
            "Episode 423, Total Reward: 500.0, Epsilon: 0.1194\n",
            "Episode 424, Total Reward: 500.0, Epsilon: 0.1188\n",
            "Episode 425, Total Reward: 349.0, Epsilon: 0.1182\n",
            "Episode 426, Total Reward: 500.0, Epsilon: 0.1176\n",
            "Episode 427, Total Reward: 500.0, Epsilon: 0.1170\n",
            "Episode 428, Total Reward: 500.0, Epsilon: 0.1164\n",
            "Episode 429, Total Reward: 388.0, Epsilon: 0.1159\n",
            "Episode 430, Total Reward: 500.0, Epsilon: 0.1153\n",
            "Episode 431, Total Reward: 500.0, Epsilon: 0.1147\n",
            "Episode 432, Total Reward: 13.0, Epsilon: 0.1141\n",
            "Episode 433, Total Reward: 236.0, Epsilon: 0.1136\n",
            "Episode 434, Total Reward: 309.0, Epsilon: 0.1130\n",
            "Episode 435, Total Reward: 365.0, Epsilon: 0.1124\n",
            "Episode 436, Total Reward: 351.0, Epsilon: 0.1119\n",
            "Episode 437, Total Reward: 366.0, Epsilon: 0.1113\n",
            "Episode 438, Total Reward: 293.0, Epsilon: 0.1107\n",
            "Episode 439, Total Reward: 159.0, Epsilon: 0.1102\n",
            "Episode 440, Total Reward: 182.0, Epsilon: 0.1096\n",
            "Episode 441, Total Reward: 376.0, Epsilon: 0.1091\n",
            "Episode 442, Total Reward: 500.0, Epsilon: 0.1085\n",
            "Episode 443, Total Reward: 416.0, Epsilon: 0.1080\n",
            "Episode 444, Total Reward: 500.0, Epsilon: 0.1075\n",
            "Episode 445, Total Reward: 477.0, Epsilon: 0.1069\n",
            "Episode 446, Total Reward: 500.0, Epsilon: 0.1064\n",
            "Episode 447, Total Reward: 85.0, Epsilon: 0.1059\n",
            "Episode 448, Total Reward: 55.0, Epsilon: 0.1053\n",
            "Episode 449, Total Reward: 474.0, Epsilon: 0.1048\n",
            "Episode 450, Total Reward: 263.0, Epsilon: 0.1043\n",
            "Episode 451, Total Reward: 500.0, Epsilon: 0.1038\n",
            "Episode 452, Total Reward: 398.0, Epsilon: 0.1032\n",
            "Episode 453, Total Reward: 225.0, Epsilon: 0.1027\n",
            "Episode 454, Total Reward: 197.0, Epsilon: 0.1022\n",
            "Episode 455, Total Reward: 500.0, Epsilon: 0.1017\n",
            "Episode 456, Total Reward: 326.0, Epsilon: 0.1012\n",
            "Episode 457, Total Reward: 215.0, Epsilon: 0.1007\n",
            "Episode 458, Total Reward: 500.0, Epsilon: 0.1002\n",
            "Episode 459, Total Reward: 500.0, Epsilon: 0.0997\n",
            "Episode 460, Total Reward: 291.0, Epsilon: 0.0992\n",
            "Episode 461, Total Reward: 500.0, Epsilon: 0.0987\n",
            "Episode 462, Total Reward: 67.0, Epsilon: 0.0982\n",
            "Episode 463, Total Reward: 500.0, Epsilon: 0.0977\n",
            "Episode 464, Total Reward: 265.0, Epsilon: 0.0972\n",
            "Episode 465, Total Reward: 118.0, Epsilon: 0.0967\n",
            "Episode 466, Total Reward: 117.0, Epsilon: 0.0962\n",
            "Episode 467, Total Reward: 144.0, Epsilon: 0.0958\n",
            "Episode 468, Total Reward: 41.0, Epsilon: 0.0953\n",
            "Episode 469, Total Reward: 106.0, Epsilon: 0.0948\n",
            "Episode 470, Total Reward: 92.0, Epsilon: 0.0943\n",
            "Episode 471, Total Reward: 500.0, Epsilon: 0.0939\n",
            "Episode 472, Total Reward: 189.0, Epsilon: 0.0934\n",
            "Episode 473, Total Reward: 97.0, Epsilon: 0.0929\n",
            "Episode 474, Total Reward: 125.0, Epsilon: 0.0925\n",
            "Episode 475, Total Reward: 26.0, Epsilon: 0.0920\n",
            "Episode 476, Total Reward: 94.0, Epsilon: 0.0915\n",
            "Episode 477, Total Reward: 500.0, Epsilon: 0.0911\n",
            "Episode 478, Total Reward: 48.0, Epsilon: 0.0906\n",
            "Episode 479, Total Reward: 500.0, Epsilon: 0.0902\n",
            "Episode 480, Total Reward: 15.0, Epsilon: 0.0897\n",
            "Episode 481, Total Reward: 500.0, Epsilon: 0.0893\n",
            "Episode 482, Total Reward: 500.0, Epsilon: 0.0888\n",
            "Episode 483, Total Reward: 254.0, Epsilon: 0.0884\n",
            "Episode 484, Total Reward: 500.0, Epsilon: 0.0879\n",
            "Episode 485, Total Reward: 500.0, Epsilon: 0.0875\n",
            "Episode 486, Total Reward: 500.0, Epsilon: 0.0871\n",
            "Episode 487, Total Reward: 500.0, Epsilon: 0.0866\n",
            "Episode 488, Total Reward: 500.0, Epsilon: 0.0862\n",
            "Episode 489, Total Reward: 33.0, Epsilon: 0.0858\n",
            "Episode 490, Total Reward: 500.0, Epsilon: 0.0853\n",
            "Episode 491, Total Reward: 119.0, Epsilon: 0.0849\n",
            "Episode 492, Total Reward: 136.0, Epsilon: 0.0845\n",
            "Episode 493, Total Reward: 500.0, Epsilon: 0.0841\n",
            "Episode 494, Total Reward: 360.0, Epsilon: 0.0836\n",
            "Episode 495, Total Reward: 116.0, Epsilon: 0.0832\n",
            "Episode 496, Total Reward: 300.0, Epsilon: 0.0828\n",
            "Episode 497, Total Reward: 143.0, Epsilon: 0.0824\n",
            "Episode 498, Total Reward: 66.0, Epsilon: 0.0820\n",
            "Episode 499, Total Reward: 381.0, Epsilon: 0.0816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynaakQdGu1tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NeYexim6u1wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WpiVUr-u1zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrDyEfpiufDE",
        "outputId": "cf857055-05fa-4ccc-f9fa-aaa1253b8d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Define the neural network model\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXWrXzKnufDH"
      },
      "outputs": [],
      "source": [
        "# Define the DQN agent class\n",
        "class DQNAgent:\n",
        "    # Initialize the DQN agent\n",
        "    def __init__(self, state_size, action_size, seed, lr):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr)\n",
        "\n",
        "        self.memory = ReplayBuffer(action_size, buffer_size=int(1e5), batch_size=64, seed=seed)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > 64:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, gamma=0.99)\n",
        "\n",
        "    # Choose an action based on the current state\n",
        "    def act(self, state, eps=0.):\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state_tensor)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if np.random.random() > eps:\n",
        "            return action_values.argmax(dim=1).item()\n",
        "        else:\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "    # Learn from batch of experiences\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1zSfMFNufDI",
        "outputId": "cf5b1319-e90b-49a7-cc2f-76c4d6b07eee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/talha/.local/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
            "  import distutils.spawn\n"
          ]
        }
      ],
      "source": [
        "# Initialize the environment and the agent\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Set up the environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Define training parameters\n",
        "num_episodes = 250\n",
        "max_steps_per_episode = 200\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.2\n",
        "epsilon_decay_rate = 0.99\n",
        "gamma = 0.9\n",
        "lr = 0.0025\n",
        "buffer_size = 10000\n",
        "buffer = deque(maxlen=buffer_size)\n",
        "batch_size = 128\n",
        "update_frequency = 10\n",
        "\n",
        "\n",
        "# Initialize the DQNAgent\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "new_agent = DQNAgent(input_dim, output_dim, seed=170715, lr = lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TJAnd7_ufDJ",
        "outputId": "34cb7e8e-caab-4bc6-8f81-7455a7bae0a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10: Finished training\n",
            "Episode 20: Finished training\n",
            "Episode 30: Finished training\n",
            "Episode 40: Finished training\n",
            "Episode 50: Finished training\n",
            "Episode 60: Finished training\n",
            "Episode 70: Finished training\n",
            "Episode 80: Finished training\n",
            "Episode 90: Finished training\n",
            "Episode 100: Finished training\n",
            "Episode 110: Finished training\n",
            "Episode 120: Finished training\n",
            "Episode 130: Finished training\n",
            "Episode 140: Finished training\n",
            "Episode 150: Finished training\n",
            "Episode 160: Finished training\n",
            "Episode 170: Finished training\n",
            "Episode 180: Finished training\n",
            "Episode 190: Finished training\n",
            "Episode 200: Finished training\n",
            "Episode 210: Finished training\n",
            "Episode 220: Finished training\n",
            "Episode 230: Finished training\n",
            "Episode 240: Finished training\n",
            "Episode 250: Finished training\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
        "\n",
        "    # Run one episode\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Choose and perform an action\n",
        "        action = new_agent.act(state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = random.sample(buffer, batch_size)\n",
        "            # Update the agent's knowledge\n",
        "            new_agent.learn(batch, gamma)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Check if the episode has ended\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if (episode + 1) % update_frequency == 0:\n",
        "        print(f\"Episode {episode + 1}: Finished training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ReIIntLufDJ",
        "outputId": "09d6a179-0526-4b1a-e10b-dea4b7692ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward over 100 test episodes: 178.75\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the agent's performance\n",
        "test_episodes = 100\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = new_agent.act(state, eps=0.)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "average_reward = sum(episode_rewards) / test_episodes\n",
        "print(f\"Average reward over {test_episodes} test episodes: {average_reward:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Flgm_yWJufDK"
      },
      "outputs": [],
      "source": [
        "# Visualize the agent's performance\n",
        "import time\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = new_agent.act(state, eps=0.)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    time.sleep(0.1)  # Add a delay to make the visualization easier to follow\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}